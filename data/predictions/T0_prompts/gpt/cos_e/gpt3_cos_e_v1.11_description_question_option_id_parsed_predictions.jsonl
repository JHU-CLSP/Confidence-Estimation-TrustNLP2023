{"id": "080ef6941410139d6869e78122bc741e", "untokenized_input": "Pick the option in line with common sense to answer the question.\nQuestion: A beaver is know for building prowess, their supplies come from where?\nOptions:\n\nA. british columbia\n\nB. body of water\n\nC. wooded area\n\nD. pay debts\n\nE. zoo", "untokenized_target": "C", "untokenized_answer_choices": ["a", "b", "c", "d", "e"], "tokenized_input": [31686, 262, 3038, 287, 1627, 351, 2219, 2565, 284, 3280, 262, 1808, 13, 198, 24361, 25, 317, 307, 8770, 318, 760, 329, 2615, 30721, 11, 511, 9416, 1282, 422, 810, 30, 198, 29046, 25, 198, 198, 32, 13, 275, 799, 680, 951, 2178, 544, 198, 198, 33, 13, 1767, 286, 1660, 198, 198, 34, 13, 4898, 276, 1989, 198, 198, 35, 13, 1414, 20250, 198, 198, 36, 13, 26626], "tokenized_answer_choices": [[32], [33], [34], [35], [36]], "target_index": 2, "api_call_parameters": {"model": "text-davinci-002", "prompt": "Pick the option in line with common sense to answer the question.\nQuestion: A beaver is know for building prowess, their supplies come from where?\nOptions:\n\nA. british columbia\n\nB. body of water\n\nC. wooded area\n\nD. pay debts\n\nE. zoo\n\n", "stop": ["<|endoftext|>"], "temperature": 0, "logprobs": 1}, "gpt3_response": {"id": "cmpl-6JWE78j3Adm5pGSbOwLVVBldAovQv", "object": "text_completion", "created": 1670110131, "model": "text-davinci-002", "choices": [{"text": "\n\nC. wooded area", "index": 0, "logprobs": {"tokens": ["\n", "\n", "C", ".", " wood", "ed", " area", "<|endoftext|>"], "token_logprobs": [-2.2958224e-05, -0.13874857, -0.0074286126, -0.00014742525, -0.29616156, -1.1514188e-05, -0.00039485365, -0.013839711], "top_logprobs": [{"\n": -2.2958224e-05}, {"\n": -0.13874857}, {"C": -0.0074286126}, {".": -0.00014742525}, {" wood": -0.29616156}, {"ed": -1.1514188e-05}, {" area": -0.00039485365}, {"<|endoftext|>": -0.013839711}], "text_offset": [234, 235, 236, 237, 238, 243, 245, 250]}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 70, "completion_tokens": 7, "total_tokens": 77}}, "parsed_prediction": "c", "parsed_prediction_is_correct": 1, "prediction_as_answer_choice": "c", "prediction_as_answer_choice_index": 2, "total_logprob": -0.45675520491199995, "num_shared_ngrams": [0, 0, 1, 0, 0]}
{"id": "8ae24d3ff199077a59e0d970feb665b7", "untokenized_input": "Pick the option in line with common sense to answer the question.\nQuestion: A car was hailed to chauffeur someone to the opera house, where was it heading?\nOptions:\n\nA. go downtown\n\nB. appear suddenly\n\nC. go fast\n\nD. bottom out\n\nE. east", "untokenized_target": "A", "untokenized_answer_choices": ["a", "b", "c", "d", "e"], "tokenized_input": [31686, 262, 3038, 287, 1627, 351, 2219, 2565, 284, 3280, 262, 1808, 13, 198, 24361, 25, 317, 1097, 373, 27052, 284, 46901, 16658, 333, 2130, 284, 262, 27296, 2156, 11, 810, 373, 340, 9087, 30, 198, 29046, 25, 198, 198, 32, 13, 467, 9436, 198, 198, 33, 13, 1656, 6451, 198, 198, 34, 13, 467, 3049, 198, 198, 35, 13, 4220, 503, 198, 198, 36, 13, 7627], "tokenized_answer_choices": [[32], [33], [34], [35], [36]], "target_index": 0, "api_call_parameters": {"model": "text-davinci-002", "prompt": "Pick the option in line with common sense to answer the question.\nQuestion: A car was hailed to chauffeur someone to the opera house, where was it heading?\nOptions:\n\nA. go downtown\n\nB. appear suddenly\n\nC. go fast\n\nD. bottom out\n\nE. east\n\n", "stop": ["<|endoftext|>"], "temperature": 0, "logprobs": 1}, "gpt3_response": {"id": "cmpl-6JWE8vsShvbKyeLm8c1UjcCreIHKc", "object": "text_completion", "created": 1670110132, "model": "text-davinci-002", "choices": [{"text": "\n\nE. east", "index": 0, "logprobs": {"tokens": ["\n", "\n", "E", ".", " east", "<|endoftext|>", "Q", ":"], "token_logprobs": [-2.2159093e-06, -0.091420025, -0.24391772, -0.0004133967, -0.07708783, -4.048565e-05, -1.2896985, 0], "top_logprobs": [{"\n": -2.2159093e-06}, {"\n": -0.091420025}, {"E": -0.24391772}, {".": -0.0004133967}, {" east": -0.07708783}, {"<|endoftext|>": -4.048565e-05}, {"Q": -1.2896985}, {":": 0}], "text_offset": [238, 239, 240, 241, 242, 247, 247, 247]}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 68, "completion_tokens": 5, "total_tokens": 73}}, "parsed_prediction": "e", "parsed_prediction_is_correct": 0, "prediction_as_answer_choice": "e", "prediction_as_answer_choice_index": 4, "total_logprob": -0.4128816732593, "num_shared_ngrams": [0, 0, 0, 0, 1]}
{"id": "9c784727afd7176b54764055df7a7927", "untokenized_input": "Pick the option in line with common sense to answer the question.\nQuestion: A child wants to play, what would they likely want?\nOptions:\n\nA. fall down\n\nB. breathe\n\nC. play tag\n\nD. be dismembered by a chainsaw\n\nE. become adult", "untokenized_target": "C", "untokenized_answer_choices": ["a", "b", "c", "d", "e"], "tokenized_input": [31686, 262, 3038, 287, 1627, 351, 2219, 2565, 284, 3280, 262, 1808, 13, 198, 24361, 25, 317, 1200, 3382, 284, 711, 11, 644, 561, 484, 1884, 765, 30, 198, 29046, 25, 198, 198, 32, 13, 2121, 866, 198, 198, 33, 13, 18044, 198, 198, 34, 13, 711, 7621, 198, 198, 35, 13, 307, 595, 11883, 9451, 416, 257, 14659, 707, 198, 198, 36, 13, 1716, 4044], "tokenized_answer_choices": [[32], [33], [34], [35], [36]], "target_index": 2, "api_call_parameters": {"model": "text-davinci-002", "prompt": "Pick the option in line with common sense to answer the question.\nQuestion: A child wants to play, what would they likely want?\nOptions:\n\nA. fall down\n\nB. breathe\n\nC. play tag\n\nD. be dismembered by a chainsaw\n\nE. become adult\n\n", "stop": ["<|endoftext|>"], "temperature": 0, "logprobs": 1}, "gpt3_response": {"id": "cmpl-6JWE8PXHZPHH8rDTqj8MndUMcWeA7", "object": "text_completion", "created": 1670110132, "model": "text-davinci-002", "choices": [{"text": "\nC. play tag", "index": 0, "logprobs": {"tokens": ["\n", "C", ".", " play", " tag", "<|endoftext|>", "Q", ":"], "token_logprobs": [-4.680184e-05, -0.41441393, -0.00041166772, -0.11246074, -0.00011499579, -0.041873872, -0.7627931, -1.1430258e-06], "top_logprobs": [{"\n": -4.680184e-05}, {"C": -0.41441393}, {".": -0.00041166772}, {" play": -0.11246074}, {" tag": -0.00011499579}, {"<|endoftext|>": -0.041873872}, {"Q": -0.7627931}, {":": -1.1430258e-06}], "text_offset": [227, 228, 229, 230, 235, 239, 239, 239]}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 67, "completion_tokens": 5, "total_tokens": 72}}, "parsed_prediction": "c", "parsed_prediction_is_correct": 1, "prediction_as_answer_choice": "c", "prediction_as_answer_choice_index": 2, "total_logprob": -0.56932200735, "num_shared_ngrams": [0, 0, 1, 0, 0]}
{"id": "a2aa95861ef74bf1ecfc55db505e3982", "untokenized_input": "Pick the option in line with common sense to answer the question.\nQuestion: A farmer sees a weasel in the woods, where is the farmer?\nOptions:\n\nA. chicken coop\n\nB. beach\n\nC. fairytale\n\nD. great outdoors\n\nE. corn fields", "untokenized_target": "D", "untokenized_answer_choices": ["a", "b", "c", "d", "e"], "tokenized_input": [31686, 262, 3038, 287, 1627, 351, 2219, 2565, 284, 3280, 262, 1808, 13, 198, 24361, 25, 317, 18739, 7224, 257, 356, 48038, 287, 262, 16479, 11, 810, 318, 262, 18739, 30, 198, 29046, 25, 198, 198, 32, 13, 9015, 763, 404, 198, 198, 33, 13, 10481, 198, 198, 34, 13, 3148, 20760, 1000, 198, 198, 35, 13, 1049, 24349, 198, 198, 36, 13, 11676, 7032], "tokenized_answer_choices": [[32], [33], [34], [35], [36]], "target_index": 3, "api_call_parameters": {"model": "text-davinci-002", "prompt": "Pick the option in line with common sense to answer the question.\nQuestion: A farmer sees a weasel in the woods, where is the farmer?\nOptions:\n\nA. chicken coop\n\nB. beach\n\nC. fairytale\n\nD. great outdoors\n\nE. corn fields\n\n", "stop": ["<|endoftext|>"], "temperature": 0, "logprobs": 1}, "gpt3_response": {"id": "cmpl-6JWE8r4d2sUN7NRQvBhShzQODTWoA", "object": "text_completion", "created": 1670110132, "model": "text-davinci-002", "choices": [{"text": "\n\nD. great outdoors", "index": 0, "logprobs": {"tokens": ["\n", "\n", "D", ".", " great", " outdoors", "<|endoftext|>", "\n"], "token_logprobs": [-3.076318e-07, -0.3716181, -0.07032852, -0.0002547938, -0.056461144, -9.490449e-06, -0.01427355, -2.4623806], "top_logprobs": [{"\n": -3.076318e-07}, {"\n": -0.3716181}, {"D": -0.07032852}, {".": -0.0002547938}, {" great": -0.056461144}, {" outdoors": -9.490449e-06}, {"<|endoftext|>": -0.01427355}, {"\n": -2.4623806}], "text_offset": [220, 221, 222, 223, 224, 230, 239, 239]}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 66, "completion_tokens": 6, "total_tokens": 72}}, "parsed_prediction": "d", "parsed_prediction_is_correct": 1, "prediction_as_answer_choice": "d", "prediction_as_answer_choice_index": 3, "total_logprob": -0.5129459058808, "num_shared_ngrams": [0, 0, 0, 1, 0]}
{"id": "d107d67d525a686fbd8282314d2ea33c", "untokenized_input": "Pick the option in line with common sense to answer the question.\nQuestion: A gentleman is carrying equipment for golf, what is he likely to have?\nOptions:\n\nA. club\n\nB. assembly hall\n\nC. meditation center\n\nD. meeting\n\nE. church", "untokenized_target": "A", "untokenized_answer_choices": ["a", "b", "c", "d", "e"], "tokenized_input": [31686, 262, 3038, 287, 1627, 351, 2219, 2565, 284, 3280, 262, 1808, 13, 198, 24361, 25, 317, 22427, 318, 6872, 5112, 329, 13126, 11, 644, 318, 339, 1884, 284, 423, 30, 198, 29046, 25, 198, 198, 32, 13, 3430, 198, 198, 33, 13, 10474, 6899, 198, 198, 34, 13, 16901, 3641, 198, 198, 35, 13, 3249, 198, 198, 36, 13, 4928], "tokenized_answer_choices": [[32], [33], [34], [35], [36]], "target_index": 0, "api_call_parameters": {"model": "text-davinci-002", "prompt": "Pick the option in line with common sense to answer the question.\nQuestion: A gentleman is carrying equipment for golf, what is he likely to have?\nOptions:\n\nA. club\n\nB. assembly hall\n\nC. meditation center\n\nD. meeting\n\nE. church\n\n", "stop": ["<|endoftext|>"], "temperature": 0, "logprobs": 1}, "gpt3_response": {"id": "cmpl-6JWE9stbNePzsqRKGlFvKBAvodAPb", "object": "text_completion", "created": 1670110133, "model": "text-davinci-002", "choices": [{"text": "\n\nA. club", "index": 0, "logprobs": {"tokens": ["\n", "\n", "A", ".", " club", "<|endoftext|>", "Q", ":"], "token_logprobs": [-3.076318e-07, -0.01613975, -0.005070685, -0.00024322867, -0.10812654, -0.0010795591, -1.734548, -1.688045e-05], "top_logprobs": [{"\n": -3.076318e-07}, {"\n": -0.01613975}, {"A": -0.005070685}, {".": -0.00024322867}, {" club": -0.10812654}, {"<|endoftext|>": -0.0010795591}, {"Q": -1.734548}, {":": -1.688045e-05}], "text_offset": [229, 230, 231, 232, 233, 238, 238, 238]}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 62, "completion_tokens": 5, "total_tokens": 67}}, "parsed_prediction": "a", "parsed_prediction_is_correct": 1, "prediction_as_answer_choice": "a", "prediction_as_answer_choice_index": 0, "total_logprob": -0.13066007040179997, "num_shared_ngrams": [1, 0, 0, 0, 0]}
{"id": "dc55d473c22b04877b11d584f9548194", "untokenized_input": "Pick the option in line with common sense to answer the question.\nQuestion: A human wants to submerge himself in water, what should he use?\nOptions:\n\nA. whirlpool bath\n\nB. coffee cup\n\nC. cup\n\nD. soft drink\n\nE. puddle", "untokenized_target": "A", "untokenized_answer_choices": ["a", "b", "c", "d", "e"], "tokenized_input": [31686, 262, 3038, 287, 1627, 351, 2219, 2565, 284, 3280, 262, 1808, 13, 198, 24361, 25, 317, 1692, 3382, 284, 32640, 469, 2241, 287, 1660, 11, 644, 815, 339, 779, 30, 198, 29046, 25, 198, 198, 32, 13, 348, 1901, 7742, 7837, 198, 198, 33, 13, 6891, 6508, 198, 198, 34, 13, 6508, 198, 198, 35, 13, 2705, 4144, 198, 198, 36, 13, 279, 24500], "tokenized_answer_choices": [[32], [33], [34], [35], [36]], "target_index": 0, "api_call_parameters": {"model": "text-davinci-002", "prompt": "Pick the option in line with common sense to answer the question.\nQuestion: A human wants to submerge himself in water, what should he use?\nOptions:\n\nA. whirlpool bath\n\nB. coffee cup\n\nC. cup\n\nD. soft drink\n\nE. puddle\n\n", "stop": ["<|endoftext|>"], "temperature": 0, "logprobs": 1}, "gpt3_response": {"id": "cmpl-6JWE9jUSDlm2iZI2huUk3ivID7XTw", "object": "text_completion", "created": 1670110133, "model": "text-davinci-002", "choices": [{"text": "\n\nA. whirlpool bath", "index": 0, "logprobs": {"tokens": ["\n", "\n", "A", ".", " wh", "irl", "pool", " bath", "<|endoftext|>", "Q", ":", " How", " to", " get", " the", " value"], "token_logprobs": [-1.0561456e-05, -0.2500311, -0.019256422, -0.005119091, -0.1406289, -9.985534e-05, -7.8540467e-07, -3.0473995e-05, -0.017452765, -1.0355465, 0, -1.5704932, -0.49960688, -2.6838531, -1.4907725, -2.8165553], "top_logprobs": [{"\n": -1.0561456e-05}, {"\n": -0.2500311}, {"A": -0.019256422}, {".": -0.005119091}, {" wh": -0.1406289}, {"irl": -9.985534e-05}, {"pool": -7.8540467e-07}, {" bath": -3.0473995e-05}, {"<|endoftext|>": -0.017452765}, {"Q": -1.0355465}, {":": 0}, {" How": -1.5704932}, {" to": -0.49960688}, {" get": -2.6838531}, {" the": -1.4907725}, {" value": -2.8165553}], "text_offset": [218, 219, 220, 221, 222, 225, 228, 232, 237, 237, 237, 237, 237, 237, 237, 237]}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 66, "completion_tokens": 8, "total_tokens": 74}}, "parsed_prediction": "a", "parsed_prediction_is_correct": 1, "prediction_as_answer_choice": "a", "prediction_as_answer_choice_index": 0, "total_logprob": -0.43262995419567, "num_shared_ngrams": [1, 0, 0, 0, 0]}
{"id": "43ab0ff711e60d51f943bbd2cdd6515a", "untokenized_input": "Pick the option in line with common sense to answer the question.\nQuestion: A loud machine is irritating, but many are expected where?\nOptions:\n\nA. museum\n\nB. house\n\nC. laboratory\n\nD. library\n\nE. industrial area", "untokenized_target": "E", "untokenized_answer_choices": ["a", "b", "c", "d", "e"], "tokenized_input": [31686, 262, 3038, 287, 1627, 351, 2219, 2565, 284, 3280, 262, 1808, 13, 198, 24361, 25, 317, 7812, 4572, 318, 42010, 11, 475, 867, 389, 2938, 810, 30, 198, 29046, 25, 198, 198, 32, 13, 13257, 198, 198, 33, 13, 2156, 198, 198, 34, 13, 14010, 198, 198, 35, 13, 5888, 198, 198, 36, 13, 7593, 1989], "tokenized_answer_choices": [[32], [33], [34], [35], [36]], "target_index": 4, "api_call_parameters": {"model": "text-davinci-002", "prompt": "Pick the option in line with common sense to answer the question.\nQuestion: A loud machine is irritating, but many are expected where?\nOptions:\n\nA. museum\n\nB. house\n\nC. laboratory\n\nD. library\n\nE. industrial area\n\n", "stop": ["<|endoftext|>"], "temperature": 0, "logprobs": 1}, "gpt3_response": {"id": "cmpl-6JWEAG7LQNefOcbVl2m2QMdnyLvrX", "object": "text_completion", "created": 1670110134, "model": "text-davinci-002", "choices": [{"text": "\n\nE. industrial area", "index": 0, "logprobs": {"tokens": ["\n", "\n", "E", ".", " industrial", " area", "<|endoftext|>", "Q"], "token_logprobs": [-5.1334588e-05, -0.0057711927, -0.016636154, -0.00023464282, -0.016180575, -9.580224e-05, -0.0028217488, -1.2656295], "top_logprobs": [{"\n": -5.1334588e-05}, {"\n": -0.0057711927}, {"E": -0.016636154}, {".": -0.00023464282}, {" industrial": -0.016180575}, {" area": -9.580224e-05}, {"<|endoftext|>": -0.0028217488}, {"Q": -1.2656295}], "text_offset": [213, 214, 215, 216, 217, 228, 233, 233]}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 58, "completion_tokens": 6, "total_tokens": 64}}, "parsed_prediction": "e", "parsed_prediction_is_correct": 1, "prediction_as_answer_choice": "e", "prediction_as_answer_choice_index": 4, "total_logprob": -0.041791450148, "num_shared_ngrams": [0, 0, 0, 0, 1]}
{"id": "8e1b0792e441a5d54ae47a4b24f48977", "untokenized_input": "Pick the option in line with common sense to answer the question.\nQuestion: A man takes a seat at a museum outside of Barcelona, where is he likely?\nOptions:\n\nA. in cinema\n\nB. martorell\n\nC. falling down\n\nD. show\n\nE. airplane", "untokenized_target": "B", "untokenized_answer_choices": ["a", "b", "c", "d", "e"], "tokenized_input": [31686, 262, 3038, 287, 1627, 351, 2219, 2565, 284, 3280, 262, 1808, 13, 198, 24361, 25, 317, 582, 2753, 257, 5852, 379, 257, 13257, 2354, 286, 15142, 11, 810, 318, 339, 1884, 30, 198, 29046, 25, 198, 198, 32, 13, 287, 22041, 198, 198, 33, 13, 11277, 382, 297, 198, 198, 34, 13, 7463, 866, 198, 198, 35, 13, 905, 198, 198, 36, 13, 19401], "tokenized_answer_choices": [[32], [33], [34], [35], [36]], "target_index": 1, "api_call_parameters": {"model": "text-davinci-002", "prompt": "Pick the option in line with common sense to answer the question.\nQuestion: A man takes a seat at a museum outside of Barcelona, where is he likely?\nOptions:\n\nA. in cinema\n\nB. martorell\n\nC. falling down\n\nD. show\n\nE. airplane\n\n", "stop": ["<|endoftext|>"], "temperature": 0, "logprobs": 1}, "gpt3_response": {"id": "cmpl-6JWEBJVEuy5EsFjURWdYpnxUTo7we", "object": "text_completion", "created": 1670110135, "model": "text-davinci-002", "choices": [{"text": "\n\nThe man is likely in a museum outside of Barcelona.", "index": 0, "logprobs": {"tokens": ["\n", "\n", "The", " man", " is", " likely", " in", " a", " museum", " outside", " of", " Barcelona", ".", "<|endoftext|>", "Q", ":"], "token_logprobs": [-4.277735e-07, -0.0058764126, -0.9491481, -0.36833307, -0.00032896505, -0.010606512, -0.6045864, -0.51720524, -0.007840933, -0.03165899, -0.0062677613, -3.883893e-06, -0.00036980762, -0.0009186434, -1.6404084, -3.529076e-06], "top_logprobs": [{"\n": -4.277735e-07}, {"\n": -0.0058764126}, {"The": -0.9491481}, {" man": -0.36833307}, {" is": -0.00032896505}, {" likely": -0.010606512}, {" in": -0.6045864}, {" a": -0.51720524}, {" museum": -0.007840933}, {" outside": -0.03165899}, {" of": -0.0062677613}, {" Barcelona": -3.883893e-06}, {".": -0.00036980762}, {"<|endoftext|>": -0.0009186434}, {"Q": -1.6404084}, {":": -3.529076e-06}], "text_offset": [226, 227, 228, 231, 235, 238, 245, 248, 250, 257, 265, 268, 278, 279, 279, 279]}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 66, "completion_tokens": 13, "total_tokens": 79}}, "parsed_prediction": "a", "parsed_prediction_is_correct": 0, "prediction_as_answer_choice": "a", "prediction_as_answer_choice_index": 0, "total_logprob": -2.5031451466365002, "num_shared_ngrams": [1, 0, 0, 0, 0]}
{"id": "4da33e6f4b789776acb1bc10195baa83", "untokenized_input": "Pick the option in line with common sense to answer the question.\nQuestion: A man wants air conditioning while we watches the game on Saturday, where will it likely be installed?\nOptions:\n\nA. car\n\nB. house\n\nC. offices\n\nD. park\n\nE. movie theatre", "untokenized_target": "B", "untokenized_answer_choices": ["a", "b", "c", "d", "e"], "tokenized_input": [31686, 262, 3038, 287, 1627, 351, 2219, 2565, 284, 3280, 262, 1808, 13, 198, 24361, 25, 317, 582, 3382, 1633, 21143, 981, 356, 16860, 262, 983, 319, 3909, 11, 810, 481, 340, 1884, 307, 6589, 30, 198, 29046, 25, 198, 198, 32, 13, 1097, 198, 198, 33, 13, 2156, 198, 198, 34, 13, 9730, 198, 198, 35, 13, 3952, 198, 198, 36, 13, 3807, 21421], "tokenized_answer_choices": [[32], [33], [34], [35], [36]], "target_index": 1, "api_call_parameters": {"model": "text-davinci-002", "prompt": "Pick the option in line with common sense to answer the question.\nQuestion: A man wants air conditioning while we watches the game on Saturday, where will it likely be installed?\nOptions:\n\nA. car\n\nB. house\n\nC. offices\n\nD. park\n\nE. movie theatre\n\n", "stop": ["<|endoftext|>"], "temperature": 0, "logprobs": 1}, "gpt3_response": {"id": "cmpl-6JWEBdzAMAd2SjaTiNV3cenWXvIra", "object": "text_completion", "created": 1670110135, "model": "text-davinci-002", "choices": [{"text": "\n\nB. house", "index": 0, "logprobs": {"tokens": ["\n", "\n", "B", ".", " house", "<|endoftext|>", "Q", ":"], "token_logprobs": [-8.296486e-06, -0.047270402, -0.009642408, -0.00017937786, -0.041494243, -0.0013418386, -2.1627228, -4.1232483e-06], "top_logprobs": [{"\n": -8.296486e-06}, {"\n": -0.047270402}, {"B": -0.009642408}, {".": -0.00017937786}, {" house": -0.041494243}, {"<|endoftext|>": -0.0013418386}, {"Q": -2.1627228}, {":": -4.1232483e-06}], "text_offset": [246, 247, 248, 249, 250, 256, 256, 256]}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 66, "completion_tokens": 5, "total_tokens": 71}}, "parsed_prediction": "b", "parsed_prediction_is_correct": 1, "prediction_as_answer_choice": "b", "prediction_as_answer_choice_index": 1, "total_logprob": -0.099936565946, "num_shared_ngrams": [0, 1, 0, 0, 0]}
{"id": "3a3b5d4a517ef70d25eb558f1a622937", "untokenized_input": "Pick the option in line with common sense to answer the question.\nQuestion: A patriotic guy with a camera is looking for a bald eagle, what is he likely to do with the eagle if he finds one?\nOptions:\n\nA. city\n\nB. canada\n\nC. minnesota\n\nD. thermal\n\nE. photograph", "untokenized_target": "E", "untokenized_answer_choices": ["a", "b", "c", "d", "e"], "tokenized_input": [31686, 262, 3038, 287, 1627, 351, 2219, 2565, 284, 3280, 262, 1808, 13, 198, 24361, 25, 317, 33453, 3516, 351, 257, 4676, 318, 2045, 329, 257, 28273, 31176, 11, 644, 318, 339, 1884, 284, 466, 351, 262, 31176, 611, 339, 7228, 530, 30, 198, 29046, 25, 198, 198, 32, 13, 1748, 198, 198, 33, 13, 460, 4763, 198, 198, 34, 13, 949, 8360, 198, 198, 35, 13, 18411, 198, 198, 36, 13, 8408], "tokenized_answer_choices": [[32], [33], [34], [35], [36]], "target_index": 4, "api_call_parameters": {"model": "text-davinci-002", "prompt": "Pick the option in line with common sense to answer the question.\nQuestion: A patriotic guy with a camera is looking for a bald eagle, what is he likely to do with the eagle if he finds one?\nOptions:\n\nA. city\n\nB. canada\n\nC. minnesota\n\nD. thermal\n\nE. photograph\n\n", "stop": ["<|endoftext|>"], "temperature": 0, "logprobs": 1}, "gpt3_response": {"id": "cmpl-6JWECu5g3yZw0mXM6zaA1rFitD1Wh", "object": "text_completion", "created": 1670110136, "model": "text-davinci-002", "choices": [{"text": "\n\nE. photograph", "index": 0, "logprobs": {"tokens": ["\n", "\n", "E", ".", " photograph", "<|endoftext|>", "Q", ":"], "token_logprobs": [-0.0027293426, -0.11528018, -0.014195862, -0.0004225786, -0.38163477, -0.0020042618, -1.110766, -6.2699432e-06], "top_logprobs": [{"\n": -0.0027293426}, {"\n": -0.11528018}, {"E": -0.014195862}, {".": -0.0004225786}, {" photograph": -0.38163477}, {"<|endoftext|>": -0.0020042618}, {"Q": -1.110766}, {":": -6.2699432e-06}], "text_offset": [262, 263, 264, 265, 266, 277, 277, 277]}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 74, "completion_tokens": 5, "total_tokens": 79}}, "parsed_prediction": "e", "parsed_prediction_is_correct": 1, "prediction_as_answer_choice": "e", "prediction_as_answer_choice_index": 4, "total_logprob": -0.5162669950000001, "num_shared_ngrams": [0, 0, 0, 0, 1]}
