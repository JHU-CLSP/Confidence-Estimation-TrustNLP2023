{"id": "080ef6941410139d6869e78122bc741e", "untokenized_input": "Pick the option in line with common sense to answer the question.\nQuestions: A beaver is know for building prowess, their supplies come from where?\nOptions:\n- british columbia\n- body of water\n- wooded area\n- pay debts\n- zoo", "untokenized_target": "wooded area", "untokenized_answer_choices": ["british columbia", "body of water", "wooded area", "pay debts", "zoo"], "tokenized_input": [31686, 262, 3038, 287, 1627, 351, 2219, 2565, 284, 3280, 262, 1808, 13, 198, 35741, 25, 317, 307, 8770, 318, 760, 329, 2615, 30721, 11, 511, 9416, 1282, 422, 810, 30, 198, 29046, 25, 198, 12, 275, 799, 680, 951, 2178, 544, 198, 12, 1767, 286, 1660, 198, 12, 4898, 276, 1989, 198, 12, 1414, 20250, 198, 12, 26626], "tokenized_answer_choices": [[65, 799, 680, 951, 2178, 544], [2618, 286, 1660, 50256, 50256, 50256], [3822, 276, 1989, 50256, 50256, 50256], [15577, 20250, 50256, 50256, 50256, 50256], [89, 2238, 50256, 50256, 50256, 50256]], "target_index": 2, "api_call_parameters": {"model": "text-davinci-002", "prompt": "Pick the option in line with common sense to answer the question.\nQuestions: A beaver is know for building prowess, their supplies come from where?\nOptions:\n- british columbia\n- body of water\n- wooded area\n- pay debts\n- zoo\n\n", "stop": ["<|endoftext|>"], "temperature": 0, "logprobs": 1}, "gpt3_response": {"id": "cmpl-6KcCaW39yajZDwvJ7pWTM7VLth6fP", "object": "text_completion", "created": 1670371428, "model": "text-davinci-002", "choices": [{"text": "\n\n- wooded area", "index": 0, "logprobs": {"tokens": ["\n", "\n", "-", " wood", "ed", " area", "<|endoftext|>", "Q"], "token_logprobs": [-3.2906587e-06, -0.34161475, -0.5667087, -0.34938607, -4.0040345e-06, -0.00023368915, -0.0023513231, -1.4357191], "top_logprobs": [{"\n": -3.2906587e-06}, {"\n": -0.34161475}, {"-": -0.5667087}, {" wood": -0.34938607}, {"ed": -4.0040345e-06}, {" area": -0.00023368915}, {"<|endoftext|>": -0.0023513231}, {"Q": -1.4357191}], "text_offset": [225, 226, 227, 228, 233, 235, 240, 240]}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 60, "completion_tokens": 6, "total_tokens": 66}}, "parsed_prediction": "wooded area", "parsed_prediction_is_correct": 1, "prediction_as_answer_choice": "wooded area", "prediction_as_answer_choice_index": 2, "total_logprob": -1.2603018269432, "num_shared_ngrams": [0, 0, 3, 0, 0]}
{"id": "8ae24d3ff199077a59e0d970feb665b7", "untokenized_input": "Pick the option in line with common sense to answer the question.\nQuestions: A car was hailed to chauffeur someone to the opera house, where was it heading?\nOptions:\n- go downtown\n- appear suddenly\n- go fast\n- bottom out\n- east", "untokenized_target": "go downtown", "untokenized_answer_choices": ["go downtown", "appear suddenly", "go fast", "bottom out", "east"], "tokenized_input": [31686, 262, 3038, 287, 1627, 351, 2219, 2565, 284, 3280, 262, 1808, 13, 198, 35741, 25, 317, 1097, 373, 27052, 284, 46901, 16658, 333, 2130, 284, 262, 27296, 2156, 11, 810, 373, 340, 9087, 30, 198, 29046, 25, 198, 12, 467, 9436, 198, 12, 1656, 6451, 198, 12, 467, 3049, 198, 12, 4220, 503, 198, 12, 7627], "tokenized_answer_choices": [[2188, 9436, 50256], [1324, 451, 6451], [2188, 3049, 50256], [22487, 503, 50256], [23316, 50256, 50256]], "target_index": 0, "api_call_parameters": {"model": "text-davinci-002", "prompt": "Pick the option in line with common sense to answer the question.\nQuestions: A car was hailed to chauffeur someone to the opera house, where was it heading?\nOptions:\n- go downtown\n- appear suddenly\n- go fast\n- bottom out\n- east\n\n", "stop": ["<|endoftext|>"], "temperature": 0, "logprobs": 1}, "gpt3_response": {"id": "cmpl-6KcCbSGBMFjSg1vWHhb29Bgfg93Lb", "object": "text_completion", "created": 1670371429, "model": "text-davinci-002", "choices": [{"text": "\n\ngo downtown", "index": 0, "logprobs": {"tokens": ["\n", "\n", "go", " downtown", "<|endoftext|>", "Q", ":", " How"], "token_logprobs": [-4.9586424e-06, -1.0466026, -0.9934807, -0.031365022, -1.2708151e-05, -1.8115504, -3.076318e-07, -1.5959234], "top_logprobs": [{"\n": -4.9586424e-06}, {"\n": -1.0466026}, {"go": -0.9934807}, {" downtown": -0.031365022}, {"<|endoftext|>": -1.2708151e-05}, {"Q": -1.8115504}, {":": -3.076318e-07}, {" How": -1.5959234}], "text_offset": [229, 230, 231, 233, 242, 242, 242, 242]}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 58, "completion_tokens": 4, "total_tokens": 62}}, "parsed_prediction": "go downtown", "parsed_prediction_is_correct": 1, "prediction_as_answer_choice": "go downtown", "prediction_as_answer_choice_index": 0, "total_logprob": -2.0714659887934, "num_shared_ngrams": [3, 0, 1, 0, 0]}
{"id": "9c784727afd7176b54764055df7a7927", "untokenized_input": "Pick the option in line with common sense to answer the question.\nQuestions: A child wants to play, what would they likely want?\nOptions:\n- fall down\n- breathe\n- play tag\n- be dismembered by a chainsaw\n- become adult", "untokenized_target": "play tag", "untokenized_answer_choices": ["fall down", "breathe", "play tag", "be dismembered by a chainsaw", "become adult"], "tokenized_input": [31686, 262, 3038, 287, 1627, 351, 2219, 2565, 284, 3280, 262, 1808, 13, 198, 35741, 25, 317, 1200, 3382, 284, 711, 11, 644, 561, 484, 1884, 765, 30, 198, 29046, 25, 198, 12, 2121, 866, 198, 12, 18044, 198, 12, 711, 7621, 198, 12, 307, 595, 11883, 9451, 416, 257, 14659, 707, 198, 12, 1716, 4044], "tokenized_answer_choices": [[7207, 866, 50256, 50256, 50256, 50256, 50256, 50256], [65, 630, 258, 50256, 50256, 50256, 50256, 50256], [1759, 7621, 50256, 50256, 50256, 50256, 50256, 50256], [1350, 595, 11883, 9451, 416, 257, 14659, 707], [9423, 462, 4044, 50256, 50256, 50256, 50256, 50256]], "target_index": 2, "api_call_parameters": {"model": "text-davinci-002", "prompt": "Pick the option in line with common sense to answer the question.\nQuestions: A child wants to play, what would they likely want?\nOptions:\n- fall down\n- breathe\n- play tag\n- be dismembered by a chainsaw\n- become adult\n\n", "stop": ["<|endoftext|>"], "temperature": 0, "logprobs": 1}, "gpt3_response": {"id": "cmpl-6KcCcBgiGx0t9v13L9IXT3Q6PReuX", "object": "text_completion", "created": 1670371430, "model": "text-davinci-002", "choices": [{"text": "\n\nplay tag", "index": 0, "logprobs": {"tokens": ["\n", "\n", "play", " tag", "<|endoftext|>", "Q", ":", " How"], "token_logprobs": [-3.2906587e-06, -0.50614315, -0.5305275, -0.000118931544, -5.4075444e-05, -1.1689073, 0, -1.5868094], "top_logprobs": [{"\n": -3.2906587e-06}, {"\n": -0.50614315}, {"play": -0.5305275}, {" tag": -0.000118931544}, {"<|endoftext|>": -5.4075444e-05}, {"Q": -1.1689073}, {":": 0}, {" How": -1.5868094}], "text_offset": [218, 219, 220, 224, 228, 228, 228, 228]}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 57, "completion_tokens": 4, "total_tokens": 61}}, "parsed_prediction": "play tag", "parsed_prediction_is_correct": 1, "prediction_as_answer_choice": "play tag", "prediction_as_answer_choice_index": 2, "total_logprob": -1.0368469476467, "num_shared_ngrams": [0, 0, 3, 0, 0]}
{"id": "a2aa95861ef74bf1ecfc55db505e3982", "untokenized_input": "Pick the option in line with common sense to answer the question.\nQuestions: A farmer sees a weasel in the woods, where is the farmer?\nOptions:\n- chicken coop\n- beach\n- fairytale\n- great outdoors\n- corn fields", "untokenized_target": "great outdoors", "untokenized_answer_choices": ["chicken coop", "beach", "fairytale", "great outdoors", "corn fields"], "tokenized_input": [31686, 262, 3038, 287, 1627, 351, 2219, 2565, 284, 3280, 262, 1808, 13, 198, 35741, 25, 317, 18739, 7224, 257, 356, 48038, 287, 262, 16479, 11, 810, 318, 262, 18739, 30, 198, 29046, 25, 198, 12, 9015, 763, 404, 198, 12, 10481, 198, 12, 3148, 20760, 1000, 198, 12, 1049, 24349, 198, 12, 11676, 7032], "tokenized_answer_choices": [[354, 5973, 763, 404], [1350, 620, 50256, 50256], [69, 13021, 29429, 50256], [18223, 24349, 50256, 50256], [20772, 7032, 50256, 50256]], "target_index": 3, "api_call_parameters": {"model": "text-davinci-002", "prompt": "Pick the option in line with common sense to answer the question.\nQuestions: A farmer sees a weasel in the woods, where is the farmer?\nOptions:\n- chicken coop\n- beach\n- fairytale\n- great outdoors\n- corn fields\n\n", "stop": ["<|endoftext|>"], "temperature": 0, "logprobs": 1}, "gpt3_response": {"id": "cmpl-6KcCcsM8PBnMTyGRexO50oJdWGOVc", "object": "text_completion", "created": 1670371430, "model": "text-davinci-002", "choices": [{"text": "\n\nThe farmer is in the great outdoors.", "index": 0, "logprobs": {"tokens": ["\n", "\n", "The", " farmer", " is", " in", " the", " great", " outdoors", ".", "<|endoftext|>", "\n", "\n", "The", " following", " questions"], "token_logprobs": [-1.4377073e-05, -0.13091853, -0.210883, -0.5980567, -0.000921388, -0.022376321, -1.8601542e-06, -0.00071462855, -5.460492e-07, -0.0013689947, -0.00022915828, -2.3055928, -0.39972636, -3.1051831, -3.1045947, -1.6369635], "top_logprobs": [{"\n": -1.4377073e-05}, {"\n": -0.13091853}, {"The": -0.210883}, {" farmer": -0.5980567}, {" is": -0.000921388}, {" in": -0.022376321}, {" the": -1.8601542e-06}, {" great": -0.00071462855}, {" outdoors": -5.460492e-07}, {".": -0.0013689947}, {"<|endoftext|>": -0.00022915828}, {"\n": -2.3055928}, {"\n": -0.39972636}, {"The": -3.1051831}, {" following": -3.1045947}, {" questions": -1.6369635}], "text_offset": [211, 212, 213, 216, 223, 226, 229, 233, 239, 248, 249, 249, 249, 249, 249, 249]}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 56, "completion_tokens": 10, "total_tokens": 66}}, "parsed_prediction": "great outdoors", "parsed_prediction_is_correct": 1, "prediction_as_answer_choice": "great outdoors", "prediction_as_answer_choice_index": 3, "total_logprob": -0.9654855038064, "num_shared_ngrams": [0, 0, 0, 3, 0]}
{"id": "d107d67d525a686fbd8282314d2ea33c", "untokenized_input": "Pick the option in line with common sense to answer the question.\nQuestions: A gentleman is carrying equipment for golf, what is he likely to have?\nOptions:\n- club\n- assembly hall\n- meditation center\n- meeting\n- church", "untokenized_target": "club", "untokenized_answer_choices": ["club", "assembly hall", "meditation center", "meeting", "church"], "tokenized_input": [31686, 262, 3038, 287, 1627, 351, 2219, 2565, 284, 3280, 262, 1808, 13, 198, 35741, 25, 317, 22427, 318, 6872, 5112, 329, 13126, 11, 644, 318, 339, 1884, 284, 423, 30, 198, 29046, 25, 198, 12, 3430, 198, 12, 10474, 6899, 198, 12, 16901, 3641, 198, 12, 3249, 198, 12, 4928], "tokenized_answer_choices": [[18664, 50256, 50256], [41873, 6899, 50256], [1150, 3780, 3641], [1326, 13629, 50256], [36964, 50256, 50256]], "target_index": 0, "api_call_parameters": {"model": "text-davinci-002", "prompt": "Pick the option in line with common sense to answer the question.\nQuestions: A gentleman is carrying equipment for golf, what is he likely to have?\nOptions:\n- club\n- assembly hall\n- meditation center\n- meeting\n- church\n\n", "stop": ["<|endoftext|>"], "temperature": 0, "logprobs": 1}, "gpt3_response": {"id": "cmpl-6KcCd7VGRBzyIZr1AjE8sKI6hqpW3", "object": "text_completion", "created": 1670371431, "model": "text-davinci-002", "choices": [{"text": "\n\n- club", "index": 0, "logprobs": {"tokens": ["\n", "\n", "-", " club", "<|endoftext|>", "Q", ":", " How"], "token_logprobs": [-2.8138238e-06, -0.18954134, -0.72688806, -0.24428703, -0.05330884, -1.311189, -3.0707724e-05, -1.6060714], "top_logprobs": [{"\n": -2.8138238e-06}, {"\n": -0.18954134}, {"-": -0.72688806}, {" club": -0.24428703}, {"<|endoftext|>": -0.05330884}, {"Q": -1.311189}, {":": -3.0707724e-05}, {" How": -1.6060714}], "text_offset": [220, 221, 222, 223, 228, 228, 228, 228]}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 52, "completion_tokens": 4, "total_tokens": 56}}, "parsed_prediction": "club", "parsed_prediction_is_correct": 1, "prediction_as_answer_choice": "club", "prediction_as_answer_choice_index": 0, "total_logprob": -1.2140280838237998, "num_shared_ngrams": [1, 0, 0, 0, 0]}
{"id": "dc55d473c22b04877b11d584f9548194", "untokenized_input": "Pick the option in line with common sense to answer the question.\nQuestions: A human wants to submerge himself in water, what should he use?\nOptions:\n- whirlpool bath\n- coffee cup\n- cup\n- soft drink\n- puddle", "untokenized_target": "whirlpool bath", "untokenized_answer_choices": ["whirlpool bath", "coffee cup", "cup", "soft drink", "puddle"], "tokenized_input": [31686, 262, 3038, 287, 1627, 351, 2219, 2565, 284, 3280, 262, 1808, 13, 198, 35741, 25, 317, 1692, 3382, 284, 32640, 469, 2241, 287, 1660, 11, 644, 815, 339, 779, 30, 198, 29046, 25, 198, 12, 348, 1901, 7742, 7837, 198, 12, 6891, 6508, 198, 12, 6508, 198, 12, 2705, 4144, 198, 12, 279, 24500], "tokenized_answer_choices": [[1929, 1901, 7742, 7837], [1073, 5853, 6508, 50256], [25244, 50256, 50256, 50256], [4215, 4144, 50256, 50256], [79, 24500, 50256, 50256]], "target_index": 0, "api_call_parameters": {"model": "text-davinci-002", "prompt": "Pick the option in line with common sense to answer the question.\nQuestions: A human wants to submerge himself in water, what should he use?\nOptions:\n- whirlpool bath\n- coffee cup\n- cup\n- soft drink\n- puddle\n\n", "stop": ["<|endoftext|>"], "temperature": 0, "logprobs": 1}, "gpt3_response": {"id": "cmpl-6KcCe2Xrjg4oz4hyntZFMt40vqzYA", "object": "text_completion", "created": 1670371432, "model": "text-davinci-002", "choices": [{"text": "\n\nwhirlpool bath", "index": 0, "logprobs": {"tokens": ["\n", "\n", "wh", "irl", "pool", " bath", "<|endoftext|>", "Q"], "token_logprobs": [-1.8841804e-07, -0.12353407, -0.9597422, -1.5214344e-05, -3.076318e-07, -5.197998e-06, -0.00021192935, -0.83939254], "top_logprobs": [{"\n": -1.8841804e-07}, {"\n": -0.12353407}, {"wh": -0.9597422}, {"irl": -1.5214344e-05}, {"pool": -3.076318e-07}, {" bath": -5.197998e-06}, {"<|endoftext|>": -0.00021192935}, {"Q": -0.83939254}], "text_offset": [209, 210, 211, 213, 216, 220, 225, 225]}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 56, "completion_tokens": 6, "total_tokens": 62}}, "parsed_prediction": "whirlpool bath", "parsed_prediction_is_correct": 1, "prediction_as_answer_choice": "whirlpool bath", "prediction_as_answer_choice_index": 0, "total_logprob": -1.0835091077418402, "num_shared_ngrams": [3, 0, 0, 0, 0]}
{"id": "43ab0ff711e60d51f943bbd2cdd6515a", "untokenized_input": "Pick the option in line with common sense to answer the question.\nQuestions: A loud machine is irritating, but many are expected where?\nOptions:\n- museum\n- house\n- laboratory\n- library\n- industrial area", "untokenized_target": "industrial area", "untokenized_answer_choices": ["museum", "house", "laboratory", "library", "industrial area"], "tokenized_input": [31686, 262, 3038, 287, 1627, 351, 2219, 2565, 284, 3280, 262, 1808, 13, 198, 35741, 25, 317, 7812, 4572, 318, 42010, 11, 475, 867, 389, 2938, 810, 30, 198, 29046, 25, 198, 12, 13257, 198, 12, 2156, 198, 12, 14010, 198, 12, 5888, 198, 12, 7593, 1989], "tokenized_answer_choices": [[76, 6744, 50256], [4803, 50256, 50256], [75, 4820, 2870], [32016, 50256, 50256], [31130, 1989, 50256]], "target_index": 4, "api_call_parameters": {"model": "text-davinci-002", "prompt": "Pick the option in line with common sense to answer the question.\nQuestions: A loud machine is irritating, but many are expected where?\nOptions:\n- museum\n- house\n- laboratory\n- library\n- industrial area\n\n", "stop": ["<|endoftext|>"], "temperature": 0, "logprobs": 1}, "gpt3_response": {"id": "cmpl-6KcCgXOq2U8i3jsdJ0gWoH0uoIGs9", "object": "text_completion", "created": 1670371434, "model": "text-davinci-002", "choices": [{"text": "\n\nindustrial area", "index": 0, "logprobs": {"tokens": ["\n", "\n", "industrial", " area", "<|endoftext|>", "Q", ":", " How"], "token_logprobs": [-0.000276853, -0.28286412, -0.343732, -1.3900239e-05, -4.1320116e-05, -1.1701407, -1.8841804e-07, -1.6038357], "top_logprobs": [{"\n": -0.000276853}, {"\n": -0.28286412}, {"industrial": -0.343732}, {" area": -1.3900239e-05}, {"<|endoftext|>": -4.1320116e-05}, {"Q": -1.1701407}, {":": -1.8841804e-07}, {" How": -1.6038357}], "text_offset": [204, 205, 206, 216, 221, 221, 221, 221]}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 48, "completion_tokens": 4, "total_tokens": 52}}, "parsed_prediction": "industrial area", "parsed_prediction_is_correct": 1, "prediction_as_answer_choice": "industrial area", "prediction_as_answer_choice_index": 4, "total_logprob": -0.626928193355, "num_shared_ngrams": [0, 0, 0, 0, 3]}
{"id": "8e1b0792e441a5d54ae47a4b24f48977", "untokenized_input": "Pick the option in line with common sense to answer the question.\nQuestions: A man takes a seat at a museum outside of Barcelona, where is he likely?\nOptions:\n- in cinema\n- martorell\n- falling down\n- show\n- airplane", "untokenized_target": "martorell", "untokenized_answer_choices": ["in cinema", "martorell", "falling down", "show", "airplane"], "tokenized_input": [31686, 262, 3038, 287, 1627, 351, 2219, 2565, 284, 3280, 262, 1808, 13, 198, 35741, 25, 317, 582, 2753, 257, 5852, 379, 257, 13257, 2354, 286, 15142, 11, 810, 318, 339, 1884, 30, 198, 29046, 25, 198, 12, 287, 22041, 198, 12, 11277, 382, 297, 198, 12, 7463, 866, 198, 12, 905, 198, 12, 19401], "tokenized_answer_choices": [[259, 22041, 50256], [13822, 382, 297], [7207, 278, 866], [12860, 50256, 50256], [958, 14382, 50256]], "target_index": 1, "api_call_parameters": {"model": "text-davinci-002", "prompt": "Pick the option in line with common sense to answer the question.\nQuestions: A man takes a seat at a museum outside of Barcelona, where is he likely?\nOptions:\n- in cinema\n- martorell\n- falling down\n- show\n- airplane\n\n", "stop": ["<|endoftext|>"], "temperature": 0, "logprobs": 1}, "gpt3_response": {"id": "cmpl-6KcCh4gGsCUMSqOvgANds4XHAQUKJ", "object": "text_completion", "created": 1670371435, "model": "text-davinci-002", "choices": [{"text": "\n\n- in a museum", "index": 0, "logprobs": {"tokens": ["\n", "\n", "-", " in", " a", " museum", "<|endoftext|>", "Q"], "token_logprobs": [-1.8841804e-07, -0.019499931, -1.3423579, -0.8783892, -0.36624175, -0.00043891673, -0.031573538, -0.34615827], "top_logprobs": [{"\n": -1.8841804e-07}, {"\n": -0.019499931}, {"-": -1.3423579}, {" in": -0.8783892}, {" a": -0.36624175}, {" museum": -0.00043891673}, {"<|endoftext|>": -0.031573538}, {"Q": -0.34615827}], "text_offset": [217, 218, 219, 220, 223, 225, 232, 232]}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 56, "completion_tokens": 6, "total_tokens": 62}}, "parsed_prediction": "in cinema", "parsed_prediction_is_correct": 0, "prediction_as_answer_choice": "in cinema", "prediction_as_answer_choice_index": 0, "total_logprob": -2.63850142414804, "num_shared_ngrams": [1, 0, 0, 0, 0]}
{"id": "4da33e6f4b789776acb1bc10195baa83", "untokenized_input": "Pick the option in line with common sense to answer the question.\nQuestions: A man wants air conditioning while we watches the game on Saturday, where will it likely be installed?\nOptions:\n- car\n- house\n- offices\n- park\n- movie theatre", "untokenized_target": "house", "untokenized_answer_choices": ["car", "house", "offices", "park", "movie theatre"], "tokenized_input": [31686, 262, 3038, 287, 1627, 351, 2219, 2565, 284, 3280, 262, 1808, 13, 198, 35741, 25, 317, 582, 3382, 1633, 21143, 981, 356, 16860, 262, 983, 319, 3909, 11, 810, 481, 340, 1884, 307, 6589, 30, 198, 29046, 25, 198, 12, 1097, 198, 12, 2156, 198, 12, 9730, 198, 12, 3952, 198, 12, 3807, 21421], "tokenized_answer_choices": [[7718, 50256], [4803, 50256], [2364, 1063], [20928, 50256], [41364, 21421]], "target_index": 1, "api_call_parameters": {"model": "text-davinci-002", "prompt": "Pick the option in line with common sense to answer the question.\nQuestions: A man wants air conditioning while we watches the game on Saturday, where will it likely be installed?\nOptions:\n- car\n- house\n- offices\n- park\n- movie theatre\n\n", "stop": ["<|endoftext|>"], "temperature": 0, "logprobs": 1}, "gpt3_response": {"id": "cmpl-6KcCikK16STJmutZGpvlfnuH9ituB", "object": "text_completion", "created": 1670371436, "model": "text-davinci-002", "choices": [{"text": "\n\n- house", "index": 0, "logprobs": {"tokens": ["\n", "\n", "-", " house", "<|endoftext|>", "Q", ":", " How"], "token_logprobs": [-1.8841804e-07, -0.11220865, -0.6823215, -0.024044707, -0.00017496901, -1.7775929, -1.8841804e-07, -1.5957569], "top_logprobs": [{"\n": -1.8841804e-07}, {"\n": -0.11220865}, {"-": -0.6823215}, {" house": -0.024044707}, {"<|endoftext|>": -0.00017496901}, {"Q": -1.7775929}, {":": -1.8841804e-07}, {" How": -1.5957569}], "text_offset": [237, 238, 239, 240, 246, 246, 246, 246]}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 56, "completion_tokens": 4, "total_tokens": 60}}, "parsed_prediction": "house", "parsed_prediction_is_correct": 1, "prediction_as_answer_choice": "house", "prediction_as_answer_choice_index": 1, "total_logprob": -0.8187500144280401, "num_shared_ngrams": [0, 1, 0, 0, 0]}
{"id": "3a3b5d4a517ef70d25eb558f1a622937", "untokenized_input": "Pick the option in line with common sense to answer the question.\nQuestions: A patriotic guy with a camera is looking for a bald eagle, what is he likely to do with the eagle if he finds one?\nOptions:\n- city\n- canada\n- minnesota\n- thermal\n- photograph", "untokenized_target": "photograph", "untokenized_answer_choices": ["city", "canada", "minnesota", "thermal", "photograph"], "tokenized_input": [31686, 262, 3038, 287, 1627, 351, 2219, 2565, 284, 3280, 262, 1808, 13, 198, 35741, 25, 317, 33453, 3516, 351, 257, 4676, 318, 2045, 329, 257, 28273, 31176, 11, 644, 318, 339, 1884, 284, 466, 351, 262, 31176, 611, 339, 7228, 530, 30, 198, 29046, 25, 198, 12, 1748, 198, 12, 460, 4763, 198, 12, 949, 8360, 198, 12, 18411, 198, 12, 8408], "tokenized_answer_choices": [[19205, 50256], [5171, 4763], [1084, 8360], [490, 7617], [38611, 2384]], "target_index": 4, "api_call_parameters": {"model": "text-davinci-002", "prompt": "Pick the option in line with common sense to answer the question.\nQuestions: A patriotic guy with a camera is looking for a bald eagle, what is he likely to do with the eagle if he finds one?\nOptions:\n- city\n- canada\n- minnesota\n- thermal\n- photograph\n\n", "stop": ["<|endoftext|>"], "temperature": 0, "logprobs": 1}, "gpt3_response": {"id": "cmpl-6KcCiC0SbqAV83aInaMnjjdeKXsQB", "object": "text_completion", "created": 1670371436, "model": "text-davinci-002", "choices": [{"text": "\n\nphotograph", "index": 0, "logprobs": {"tokens": ["\n", "\n", "phot", "ograph", "<|endoftext|>", "Q", ":", " How"], "token_logprobs": [-0.0018219413, -0.51143485, -0.9031857, -3.076318e-07, -0.0006817035, -1.4096192, -0.00018545937, -1.6125945], "top_logprobs": [{"\n": -0.0018219413}, {"\n": -0.51143485}, {"phot": -0.9031857}, {"ograph": -3.076318e-07}, {"<|endoftext|>": -0.0006817035}, {"Q": -1.4096192}, {":": -0.00018545937}, {" How": -1.6125945}], "text_offset": [253, 254, 255, 259, 265, 265, 265, 265]}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 64, "completion_tokens": 4, "total_tokens": 68}}, "parsed_prediction": "photograph", "parsed_prediction_is_correct": 1, "prediction_as_answer_choice": "photograph", "prediction_as_answer_choice_index": 4, "total_logprob": -1.4171245024318, "num_shared_ngrams": [0, 0, 0, 0, 1]}
