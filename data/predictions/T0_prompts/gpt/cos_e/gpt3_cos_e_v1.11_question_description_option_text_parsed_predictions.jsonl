{"id": "080ef6941410139d6869e78122bc741e", "untokenized_input": "A beaver is know for building prowess, their supplies come from where?\nChoose the most suitable option to answer the above question.\nOptions:\n- british columbia\n- body of water\n- wooded area\n- pay debts\n- zoo", "untokenized_target": "wooded area", "untokenized_answer_choices": ["british columbia", "body of water", "wooded area", "pay debts", "zoo"], "tokenized_input": [32, 307, 8770, 318, 760, 329, 2615, 30721, 11, 511, 9416, 1282, 422, 810, 30, 198, 31851, 262, 749, 11080, 3038, 284, 3280, 262, 2029, 1808, 13, 198, 29046, 25, 198, 12, 275, 799, 680, 951, 2178, 544, 198, 12, 1767, 286, 1660, 198, 12, 4898, 276, 1989, 198, 12, 1414, 20250, 198, 12, 26626], "tokenized_answer_choices": [[65, 799, 680, 951, 2178, 544], [2618, 286, 1660, 50256, 50256, 50256], [3822, 276, 1989, 50256, 50256, 50256], [15577, 20250, 50256, 50256, 50256, 50256], [89, 2238, 50256, 50256, 50256, 50256]], "target_index": 2, "api_call_parameters": {"model": "text-davinci-002", "prompt": "A beaver is know for building prowess, their supplies come from where?\nChoose the most suitable option to answer the above question.\nOptions:\n- british columbia\n- body of water\n- wooded area\n- pay debts\n- zoo\n\n", "stop": ["<|endoftext|>"], "temperature": 0, "logprobs": 1}, "gpt3_response": {"id": "cmpl-6SDetEnVoAA5tkIFLEfII0PcKKO8l", "object": "text_completion", "created": 1672183707, "model": "text-davinci-002", "choices": [{"text": "\n\nA beaver is known for building prowess, their supplies come from a", "index": 0, "logprobs": {"tokens": ["\n", "\n", "A", " be", "aver", " is", " known", " for", " building", " prowess", ",", " their", " supplies", " come", " from", " a"], "token_logprobs": [-5.3363005e-05, -0.036921944, -0.51692665, -0.0047758003, -0.00026779127, -0.59806895, -0.17568196, -6.5041706e-05, -0.08439904, -0.13784482, -0.040511824, -0.45047975, -3.01145e-05, -0.00037147748, -0.0030726488, -0.06614806], "top_logprobs": [{"\n": -5.3363005e-05}, {"\n": -0.036921944}, {"A": -0.51692665}, {" be": -0.0047758003}, {"aver": -0.00026779127}, {" is": -0.59806895}, {" known": -0.17568196}, {" for": -6.5041706e-05}, {" building": -0.08439904}, {" prowess": -0.13784482}, {",": -0.040511824}, {" their": -0.45047975}, {" supplies": -3.01145e-05}, {" come": -0.00037147748}, {" from": -0.0030726488}, {" a": -0.06614806}], "text_offset": [210, 211, 212, 213, 216, 220, 223, 229, 233, 242, 250, 251, 257, 266, 271, 276]}, "finish_reason": "length"}], "usage": {"prompt_tokens": 56, "completion_tokens": 16, "total_tokens": 72}}, "parsed_prediction": null, "parsed_prediction_is_correct": null, "prediction_as_answer_choice": null, "prediction_as_answer_choice_index": null, "total_logprob": null, "num_shared_ngrams": [0, 0, 0, 0, 0]}
{"id": "8ae24d3ff199077a59e0d970feb665b7", "untokenized_input": "A car was hailed to chauffeur someone to the opera house, where was it heading?\nChoose the most suitable option to answer the above question.\nOptions:\n- go downtown\n- appear suddenly\n- go fast\n- bottom out\n- east", "untokenized_target": "go downtown", "untokenized_answer_choices": ["go downtown", "appear suddenly", "go fast", "bottom out", "east"], "tokenized_input": [32, 1097, 373, 27052, 284, 46901, 16658, 333, 2130, 284, 262, 27296, 2156, 11, 810, 373, 340, 9087, 30, 198, 31851, 262, 749, 11080, 3038, 284, 3280, 262, 2029, 1808, 13, 198, 29046, 25, 198, 12, 467, 9436, 198, 12, 1656, 6451, 198, 12, 467, 3049, 198, 12, 4220, 503, 198, 12, 7627], "tokenized_answer_choices": [[2188, 9436, 50256], [1324, 451, 6451], [2188, 3049, 50256], [22487, 503, 50256], [23316, 50256, 50256]], "target_index": 0, "api_call_parameters": {"model": "text-davinci-002", "prompt": "A car was hailed to chauffeur someone to the opera house, where was it heading?\nChoose the most suitable option to answer the above question.\nOptions:\n- go downtown\n- appear suddenly\n- go fast\n- bottom out\n- east\n\n", "stop": ["<|endoftext|>"], "temperature": 0, "logprobs": 1}, "gpt3_response": {"id": "cmpl-6SDet0wfrFJHPusMaZkUlcKkzFUUx", "object": "text_completion", "created": 1672183707, "model": "text-davinci-002", "choices": [{"text": "\nThe car was headed to the opera house downtown.", "index": 0, "logprobs": {"tokens": ["\n", "The", " car", " was", " headed", " to", " the", " opera", " house", " downtown", ".", "<|endoftext|>", "Q"], "token_logprobs": [-6.2699432e-06, -0.7352908, -0.107644, -0.0063795676, -1.0078913, -0.054148115, -9.24644e-05, -0.0009402999, -2.2159093e-06, -0.60081124, -7.8540467e-07, -1.1430258e-06, -1.4465029], "top_logprobs": [{"\n": -6.2699432e-06}, {"The": -0.7352908}, {" car": -0.107644}, {" was": -0.0063795676}, {" headed": -1.0078913}, {" to": -0.054148115}, {" the": -9.24644e-05}, {" opera": -0.0009402999}, {" house": -2.2159093e-06}, {" downtown": -0.60081124}, {".": -7.8540467e-07}, {"<|endoftext|>": -1.1430258e-06}, {"Q": -1.4465029}], "text_offset": [214, 215, 218, 222, 226, 233, 236, 240, 246, 252, 261, 262, 262]}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 54, "completion_tokens": 11, "total_tokens": 65}}, "parsed_prediction": "go downtown", "parsed_prediction_is_correct": 1, "prediction_as_answer_choice": "go downtown", "prediction_as_answer_choice_index": 0, "total_logprob": -2.51320820118297, "num_shared_ngrams": [1, 0, 0, 0, 0]}
{"id": "9c784727afd7176b54764055df7a7927", "untokenized_input": "A child wants to play, what would they likely want?\nChoose the most suitable option to answer the above question.\nOptions:\n- fall down\n- breathe\n- play tag\n- be dismembered by a chainsaw\n- become adult", "untokenized_target": "play tag", "untokenized_answer_choices": ["fall down", "breathe", "play tag", "be dismembered by a chainsaw", "become adult"], "tokenized_input": [32, 1200, 3382, 284, 711, 11, 644, 561, 484, 1884, 765, 30, 198, 31851, 262, 749, 11080, 3038, 284, 3280, 262, 2029, 1808, 13, 198, 29046, 25, 198, 12, 2121, 866, 198, 12, 18044, 198, 12, 711, 7621, 198, 12, 307, 595, 11883, 9451, 416, 257, 14659, 707, 198, 12, 1716, 4044], "tokenized_answer_choices": [[7207, 866, 50256, 50256, 50256, 50256, 50256, 50256], [65, 630, 258, 50256, 50256, 50256, 50256, 50256], [1759, 7621, 50256, 50256, 50256, 50256, 50256, 50256], [1350, 595, 11883, 9451, 416, 257, 14659, 707], [9423, 462, 4044, 50256, 50256, 50256, 50256, 50256]], "target_index": 2, "api_call_parameters": {"model": "text-davinci-002", "prompt": "A child wants to play, what would they likely want?\nChoose the most suitable option to answer the above question.\nOptions:\n- fall down\n- breathe\n- play tag\n- be dismembered by a chainsaw\n- become adult\n\n", "stop": ["<|endoftext|>"], "temperature": 0, "logprobs": 1}, "gpt3_response": {"id": "cmpl-6SDeuQkk2uhZ2paJpAp83redbz5Dg", "object": "text_completion", "created": 1672183708, "model": "text-davinci-002", "choices": [{"text": "\nplay tag", "index": 0, "logprobs": {"tokens": ["\n", "play", " tag", "<|endoftext|>", "Q"], "token_logprobs": [-1.2946569e-05, -0.40406153, -0.00013943638, -1.5805701e-05, -0.9213992], "top_logprobs": [{"\n": -1.2946569e-05}, {"play": -0.40406153}, {" tag": -0.00013943638}, {"<|endoftext|>": -1.5805701e-05}, {"Q": -0.9213992}], "text_offset": [203, 204, 208, 212, 212]}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 53, "completion_tokens": 3, "total_tokens": 56}}, "parsed_prediction": "play tag", "parsed_prediction_is_correct": 1, "prediction_as_answer_choice": "play tag", "prediction_as_answer_choice_index": 2, "total_logprob": -0.40422971865000007, "num_shared_ngrams": [0, 0, 3, 0, 0]}
{"id": "a2aa95861ef74bf1ecfc55db505e3982", "untokenized_input": "A farmer sees a weasel in the woods, where is the farmer?\nChoose the most suitable option to answer the above question.\nOptions:\n- chicken coop\n- beach\n- fairytale\n- great outdoors\n- corn fields", "untokenized_target": "great outdoors", "untokenized_answer_choices": ["chicken coop", "beach", "fairytale", "great outdoors", "corn fields"], "tokenized_input": [32, 18739, 7224, 257, 356, 48038, 287, 262, 16479, 11, 810, 318, 262, 18739, 30, 198, 31851, 262, 749, 11080, 3038, 284, 3280, 262, 2029, 1808, 13, 198, 29046, 25, 198, 12, 9015, 763, 404, 198, 12, 10481, 198, 12, 3148, 20760, 1000, 198, 12, 1049, 24349, 198, 12, 11676, 7032], "tokenized_answer_choices": [[354, 5973, 763, 404], [1350, 620, 50256, 50256], [69, 13021, 29429, 50256], [18223, 24349, 50256, 50256], [20772, 7032, 50256, 50256]], "target_index": 3, "api_call_parameters": {"model": "text-davinci-002", "prompt": "A farmer sees a weasel in the woods, where is the farmer?\nChoose the most suitable option to answer the above question.\nOptions:\n- chicken coop\n- beach\n- fairytale\n- great outdoors\n- corn fields\n\n", "stop": ["<|endoftext|>"], "temperature": 0, "logprobs": 1}, "gpt3_response": {"id": "cmpl-6SDeuzQwyQAiXZxx3h1EcA5W5V0Vm", "object": "text_completion", "created": 1672183708, "model": "text-davinci-002", "choices": [{"text": "\n\nThe farmer is in the great outdoors.", "index": 0, "logprobs": {"tokens": ["\n", "\n", "The", " farmer", " is", " in", " the", " great", " outdoors", ".", "<|endoftext|>", "Q", ":"], "token_logprobs": [-5.79217e-06, -0.3162577, -0.0478701, -0.18326686, -0.03683363, -0.048887417, -1.7400124e-06, -0.14767045, -3.076318e-07, -0.00097233884, -6.5092986e-06, -1.5717968, 0], "top_logprobs": [{"\n": -5.79217e-06}, {"\n": -0.3162577}, {"The": -0.0478701}, {" farmer": -0.18326686}, {" is": -0.03683363}, {" in": -0.048887417}, {" the": -1.7400124e-06}, {" great": -0.14767045}, {" outdoors": -3.076318e-07}, {".": -0.00097233884}, {"<|endoftext|>": -6.5092986e-06}, {"Q": -1.5717968}, {":": 0}], "text_offset": [196, 197, 198, 201, 208, 211, 214, 218, 224, 233, 234, 234, 234]}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 52, "completion_tokens": 10, "total_tokens": 62}}, "parsed_prediction": "great outdoors", "parsed_prediction_is_correct": 1, "prediction_as_answer_choice": "great outdoors", "prediction_as_answer_choice_index": 3, "total_logprob": -0.7817728449528001, "num_shared_ngrams": [0, 0, 0, 3, 0]}
{"id": "d107d67d525a686fbd8282314d2ea33c", "untokenized_input": "A gentleman is carrying equipment for golf, what is he likely to have?\nChoose the most suitable option to answer the above question.\nOptions:\n- club\n- assembly hall\n- meditation center\n- meeting\n- church", "untokenized_target": "club", "untokenized_answer_choices": ["club", "assembly hall", "meditation center", "meeting", "church"], "tokenized_input": [32, 22427, 318, 6872, 5112, 329, 13126, 11, 644, 318, 339, 1884, 284, 423, 30, 198, 31851, 262, 749, 11080, 3038, 284, 3280, 262, 2029, 1808, 13, 198, 29046, 25, 198, 12, 3430, 198, 12, 10474, 6899, 198, 12, 16901, 3641, 198, 12, 3249, 198, 12, 4928], "tokenized_answer_choices": [[18664, 50256, 50256], [41873, 6899, 50256], [1150, 3780, 3641], [1326, 13629, 50256], [36964, 50256, 50256]], "target_index": 0, "api_call_parameters": {"model": "text-davinci-002", "prompt": "A gentleman is carrying equipment for golf, what is he likely to have?\nChoose the most suitable option to answer the above question.\nOptions:\n- club\n- assembly hall\n- meditation center\n- meeting\n- church\n\n", "stop": ["<|endoftext|>"], "temperature": 0, "logprobs": 1}, "gpt3_response": {"id": "cmpl-6SDeuIpYGf0LVNDmUC7kEws7n18mw", "object": "text_completion", "created": 1672183708, "model": "text-davinci-002", "choices": [{"text": "\nA gentleman is carrying equipment for golf, so he is likely to have a", "index": 0, "logprobs": {"tokens": ["\n", "A", " gentleman", " is", " carrying", " equipment", " for", " golf", ",", " so", " he", " is", " likely", " to", " have", " a"], "token_logprobs": [-4.2272848e-05, -0.9319208, -0.061510634, -0.09856703, -0.19016802, -0.01923497, -3.5357807e-05, -0.00018438557, -0.00038375973, -0.9444438, -2.6060465e-05, -0.040450253, -0.00040928167, -0.009640059, -0.00034190435, -0.21960898], "top_logprobs": [{"\n": -4.2272848e-05}, {"A": -0.9319208}, {" gentleman": -0.061510634}, {" is": -0.09856703}, {" carrying": -0.19016802}, {" equipment": -0.01923497}, {" for": -3.5357807e-05}, {" golf": -0.00018438557}, {",": -0.00038375973}, {" so": -0.9444438}, {" he": -2.6060465e-05}, {" is": -0.040450253}, {" likely": -0.00040928167}, {" to": -0.009640059}, {" have": -0.00034190435}, {" a": -0.21960898}], "text_offset": [205, 206, 207, 217, 220, 229, 239, 243, 248, 249, 252, 255, 258, 265, 268, 273]}, "finish_reason": "length"}], "usage": {"prompt_tokens": 48, "completion_tokens": 16, "total_tokens": 64}}, "parsed_prediction": null, "parsed_prediction_is_correct": null, "prediction_as_answer_choice": null, "prediction_as_answer_choice_index": null, "total_logprob": null, "num_shared_ngrams": [0, 0, 0, 0, 0]}
{"id": "dc55d473c22b04877b11d584f9548194", "untokenized_input": "A human wants to submerge himself in water, what should he use?\nChoose the most suitable option to answer the above question.\nOptions:\n- whirlpool bath\n- coffee cup\n- cup\n- soft drink\n- puddle", "untokenized_target": "whirlpool bath", "untokenized_answer_choices": ["whirlpool bath", "coffee cup", "cup", "soft drink", "puddle"], "tokenized_input": [32, 1692, 3382, 284, 32640, 469, 2241, 287, 1660, 11, 644, 815, 339, 779, 30, 198, 31851, 262, 749, 11080, 3038, 284, 3280, 262, 2029, 1808, 13, 198, 29046, 25, 198, 12, 348, 1901, 7742, 7837, 198, 12, 6891, 6508, 198, 12, 6508, 198, 12, 2705, 4144, 198, 12, 279, 24500], "tokenized_answer_choices": [[1929, 1901, 7742, 7837], [1073, 5853, 6508, 50256], [25244, 50256, 50256, 50256], [4215, 4144, 50256, 50256], [79, 24500, 50256, 50256]], "target_index": 0, "api_call_parameters": {"model": "text-davinci-002", "prompt": "A human wants to submerge himself in water, what should he use?\nChoose the most suitable option to answer the above question.\nOptions:\n- whirlpool bath\n- coffee cup\n- cup\n- soft drink\n- puddle\n\n", "stop": ["<|endoftext|>"], "temperature": 0, "logprobs": 1}, "gpt3_response": {"id": "cmpl-6SDevlKXhyJKtejnJfxGfaO5bZyfp", "object": "text_completion", "created": 1672183709, "model": "text-davinci-002", "choices": [{"text": "\nA whirlpool bath would be the most suitable option for a human to", "index": 0, "logprobs": {"tokens": ["\n", "A", " wh", "irl", "pool", " bath", " would", " be", " the", " most", " suitable", " option", " for", " a", " human", " to"], "token_logprobs": [-6.6619094e-07, -0.7198002, -0.6655518, -1.9263687e-05, -6.6619094e-07, -7.3446927e-06, -0.016775066, -0.0021440268, -0.0022190532, -0.6528209, -0.024009906, -0.0011028883, -0.017064717, -0.23485996, -0.006873736, -0.009792575], "top_logprobs": [{"\n": -6.6619094e-07}, {"A": -0.7198002}, {" wh": -0.6655518}, {"irl": -1.9263687e-05}, {"pool": -6.6619094e-07}, {" bath": -7.3446927e-06}, {" would": -0.016775066}, {" be": -0.0021440268}, {" the": -0.0022190532}, {" most": -0.6528209}, {" suitable": -0.024009906}, {" option": -0.0011028883}, {" for": -0.017064717}, {" a": -0.23485996}, {" human": -0.006873736}, {" to": -0.009792575}], "text_offset": [194, 195, 196, 199, 202, 206, 211, 217, 220, 224, 229, 238, 245, 249, 251, 257]}, "finish_reason": "length"}], "usage": {"prompt_tokens": 52, "completion_tokens": 16, "total_tokens": 68}}, "parsed_prediction": "whirlpool bath", "parsed_prediction_is_correct": 1, "prediction_as_answer_choice": "whirlpool bath", "prediction_as_answer_choice_index": 0, "total_logprob": -2.3530427690615796, "num_shared_ngrams": [3, 0, 0, 0, 0]}
{"id": "43ab0ff711e60d51f943bbd2cdd6515a", "untokenized_input": "A loud machine is irritating, but many are expected where?\nChoose the most suitable option to answer the above question.\nOptions:\n- museum\n- house\n- laboratory\n- library\n- industrial area", "untokenized_target": "industrial area", "untokenized_answer_choices": ["museum", "house", "laboratory", "library", "industrial area"], "tokenized_input": [32, 7812, 4572, 318, 42010, 11, 475, 867, 389, 2938, 810, 30, 198, 31851, 262, 749, 11080, 3038, 284, 3280, 262, 2029, 1808, 13, 198, 29046, 25, 198, 12, 13257, 198, 12, 2156, 198, 12, 14010, 198, 12, 5888, 198, 12, 7593, 1989], "tokenized_answer_choices": [[76, 6744, 50256], [4803, 50256, 50256], [75, 4820, 2870], [32016, 50256, 50256], [31130, 1989, 50256]], "target_index": 4, "api_call_parameters": {"model": "text-davinci-002", "prompt": "A loud machine is irritating, but many are expected where?\nChoose the most suitable option to answer the above question.\nOptions:\n- museum\n- house\n- laboratory\n- library\n- industrial area\n\n", "stop": ["<|endoftext|>"], "temperature": 0, "logprobs": 1}, "gpt3_response": {"id": "cmpl-6SDev6EdZ2mWBuAxM8QVNfYxXBH9e", "object": "text_completion", "created": 1672183709, "model": "text-davinci-002", "choices": [{"text": "\n\nindustrial area", "index": 0, "logprobs": {"tokens": ["\n", "\n", "industrial", " area", "<|endoftext|>"], "token_logprobs": [-0.000113448885, -0.20056088, -0.6014181, -1.9382902e-05, -2.4272329e-05], "top_logprobs": [{"\n": -0.000113448885}, {"\n": -0.20056088}, {"industrial": -0.6014181}, {" area": -1.9382902e-05}, {"<|endoftext|>": -2.4272329e-05}], "text_offset": [189, 190, 191, 201, 206]}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 44, "completion_tokens": 4, "total_tokens": 48}}, "parsed_prediction": "industrial area", "parsed_prediction_is_correct": 1, "prediction_as_answer_choice": "industrial area", "prediction_as_answer_choice_index": 4, "total_logprob": -0.802136084116, "num_shared_ngrams": [0, 0, 0, 0, 3]}
{"id": "8e1b0792e441a5d54ae47a4b24f48977", "untokenized_input": "A man takes a seat at a museum outside of Barcelona, where is he likely?\nChoose the most suitable option to answer the above question.\nOptions:\n- in cinema\n- martorell\n- falling down\n- show\n- airplane", "untokenized_target": "martorell", "untokenized_answer_choices": ["in cinema", "martorell", "falling down", "show", "airplane"], "tokenized_input": [32, 582, 2753, 257, 5852, 379, 257, 13257, 2354, 286, 15142, 11, 810, 318, 339, 1884, 30, 198, 31851, 262, 749, 11080, 3038, 284, 3280, 262, 2029, 1808, 13, 198, 29046, 25, 198, 12, 287, 22041, 198, 12, 11277, 382, 297, 198, 12, 7463, 866, 198, 12, 905, 198, 12, 19401], "tokenized_answer_choices": [[259, 22041, 50256], [13822, 382, 297], [7207, 278, 866], [12860, 50256, 50256], [958, 14382, 50256]], "target_index": 1, "api_call_parameters": {"model": "text-davinci-002", "prompt": "A man takes a seat at a museum outside of Barcelona, where is he likely?\nChoose the most suitable option to answer the above question.\nOptions:\n- in cinema\n- martorell\n- falling down\n- show\n- airplane\n\n", "stop": ["<|endoftext|>"], "temperature": 0, "logprobs": 1}, "gpt3_response": {"id": "cmpl-6SDevRG1iSdLADj8Lgw3UjglY6Hve", "object": "text_completion", "created": 1672183709, "model": "text-davinci-002", "choices": [{"text": "\n\nThe man is likely at a museum outside of Barcelona.", "index": 0, "logprobs": {"tokens": ["\n", "\n", "The", " man", " is", " likely", " at", " a", " museum", " outside", " of", " Barcelona", ".", "<|endoftext|>", "Q", ":"], "token_logprobs": [-4.277735e-07, -0.012362736, -0.28877127, -0.09084566, -0.0028292204, -0.02862654, -0.6688693, -0.035979282, -7.326899e-05, -0.16010027, -0.001255418, -6.6619094e-07, -0.00044380713, -3.762558e-05, -0.95425236, -6.827632e-08], "top_logprobs": [{"\n": -4.277735e-07}, {"\n": -0.012362736}, {"The": -0.28877127}, {" man": -0.09084566}, {" is": -0.0028292204}, {" likely": -0.02862654}, {" at": -0.6688693}, {" a": -0.035979282}, {" museum": -7.326899e-05}, {" outside": -0.16010027}, {" of": -0.001255418}, {" Barcelona": -6.6619094e-07}, {".": -0.00044380713}, {"<|endoftext|>": -3.762558e-05}, {"Q": -0.95425236}, {":": -6.827632e-08}], "text_offset": [202, 203, 204, 207, 211, 214, 221, 224, 226, 233, 241, 244, 254, 255, 255, 255]}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 52, "completion_tokens": 13, "total_tokens": 65}}, "parsed_prediction": null, "parsed_prediction_is_correct": null, "prediction_as_answer_choice": null, "prediction_as_answer_choice_index": null, "total_logprob": null, "num_shared_ngrams": [0, 0, 0, 0, 0]}
{"id": "4da33e6f4b789776acb1bc10195baa83", "untokenized_input": "A man wants air conditioning while we watches the game on Saturday, where will it likely be installed?\nChoose the most suitable option to answer the above question.\nOptions:\n- car\n- house\n- offices\n- park\n- movie theatre", "untokenized_target": "house", "untokenized_answer_choices": ["car", "house", "offices", "park", "movie theatre"], "tokenized_input": [32, 582, 3382, 1633, 21143, 981, 356, 16860, 262, 983, 319, 3909, 11, 810, 481, 340, 1884, 307, 6589, 30, 198, 31851, 262, 749, 11080, 3038, 284, 3280, 262, 2029, 1808, 13, 198, 29046, 25, 198, 12, 1097, 198, 12, 2156, 198, 12, 9730, 198, 12, 3952, 198, 12, 3807, 21421], "tokenized_answer_choices": [[7718, 50256], [4803, 50256], [2364, 1063], [20928, 50256], [41364, 21421]], "target_index": 1, "api_call_parameters": {"model": "text-davinci-002", "prompt": "A man wants air conditioning while we watches the game on Saturday, where will it likely be installed?\nChoose the most suitable option to answer the above question.\nOptions:\n- car\n- house\n- offices\n- park\n- movie theatre\n\n", "stop": ["<|endoftext|>"], "temperature": 0, "logprobs": 1}, "gpt3_response": {"id": "cmpl-6SDeweiadjihz8iIQa42G7fplGBCF", "object": "text_completion", "created": 1672183710, "model": "text-davinci-002", "choices": [{"text": "\nThe man will likely install the air conditioning in his house.", "index": 0, "logprobs": {"tokens": ["\n", "The", " man", " will", " likely", " install", " the", " air", " conditioning", " in", " his", " house", ".", "<|endoftext|>", "Q", ":"], "token_logprobs": [-1.022884e-06, -0.7582687, -0.6993981, -0.20887788, -0.6244066, -0.24971625, -0.28237697, -1.5006569e-06, -0.018481134, -0.008014847, -0.0027157762, -0.0017090283, -0.12622705, -1.3422466e-05, -1.491098, 0], "top_logprobs": [{"\n": -1.022884e-06}, {"The": -0.7582687}, {" man": -0.6993981}, {" will": -0.20887788}, {" likely": -0.6244066}, {" install": -0.24971625}, {" the": -0.28237697}, {" air": -1.5006569e-06}, {" conditioning": -0.018481134}, {" in": -0.008014847}, {" his": -0.0027157762}, {" house": -0.0017090283}, {".": -0.12622705}, {"<|endoftext|>": -1.3422466e-05}, {"Q": -1.491098}, {":": 0}], "text_offset": [222, 223, 226, 230, 235, 242, 250, 254, 258, 271, 274, 278, 284, 285, 285, 285]}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 52, "completion_tokens": 13, "total_tokens": 65}}, "parsed_prediction": "house", "parsed_prediction_is_correct": 1, "prediction_as_answer_choice": "house", "prediction_as_answer_choice_index": 1, "total_logprob": -2.9802082815069006, "num_shared_ngrams": [0, 1, 0, 0, 0]}
{"id": "3a3b5d4a517ef70d25eb558f1a622937", "untokenized_input": "A patriotic guy with a camera is looking for a bald eagle, what is he likely to do with the eagle if he finds one?\nChoose the most suitable option to answer the above question.\nOptions:\n- city\n- canada\n- minnesota\n- thermal\n- photograph", "untokenized_target": "photograph", "untokenized_answer_choices": ["city", "canada", "minnesota", "thermal", "photograph"], "tokenized_input": [32, 33453, 3516, 351, 257, 4676, 318, 2045, 329, 257, 28273, 31176, 11, 644, 318, 339, 1884, 284, 466, 351, 262, 31176, 611, 339, 7228, 530, 30, 198, 31851, 262, 749, 11080, 3038, 284, 3280, 262, 2029, 1808, 13, 198, 29046, 25, 198, 12, 1748, 198, 12, 460, 4763, 198, 12, 949, 8360, 198, 12, 18411, 198, 12, 8408], "tokenized_answer_choices": [[19205, 50256], [5171, 4763], [1084, 8360], [490, 7617], [38611, 2384]], "target_index": 4, "api_call_parameters": {"model": "text-davinci-002", "prompt": "A patriotic guy with a camera is looking for a bald eagle, what is he likely to do with the eagle if he finds one?\nChoose the most suitable option to answer the above question.\nOptions:\n- city\n- canada\n- minnesota\n- thermal\n- photograph\n\n", "stop": ["<|endoftext|>"], "temperature": 0, "logprobs": 1}, "gpt3_response": {"id": "cmpl-6SDewxnmZg3R24kUoPBcZjcwnj58O", "object": "text_completion", "created": 1672183710, "model": "text-davinci-002", "choices": [{"text": "\nIf the patriotic guy with a camera finds a bald eagle, he is likely", "index": 0, "logprobs": {"tokens": ["\n", "If", " the", " patriotic", " guy", " with", " a", " camera", " finds", " a", " bald", " eagle", ",", " he", " is", " likely"], "token_logprobs": [-0.0010309285, -0.9519175, -0.12966245, -0.13556363, -0.0031693925, -0.13204162, -0.3158671, -3.9292627e-05, -0.0071745175, 0, -8.296486e-06, -2.6060465e-05, -0.00016674175, -0.00057154533, -0.057499494, -0.056211453], "top_logprobs": [{"\n": -0.0010309285}, {"If": -0.9519175}, {" the": -0.12966245}, {" patriotic": -0.13556363}, {" guy": -0.0031693925}, {" with": -0.13204162}, {" a": -0.3158671}, {" camera": -3.9292627e-05}, {" finds": -0.0071745175}, {" a": 0}, {" bald": -8.296486e-06}, {" eagle": -2.6060465e-05}, {",": -0.00016674175}, {" he": -0.00057154533}, {" is": -0.057499494}, {" likely": -0.056211453}], "text_offset": [238, 239, 241, 245, 255, 259, 264, 266, 273, 279, 281, 286, 292, 293, 296, 299]}, "finish_reason": "length"}], "usage": {"prompt_tokens": 60, "completion_tokens": 16, "total_tokens": 76}}, "parsed_prediction": null, "parsed_prediction_is_correct": null, "prediction_as_answer_choice": null, "prediction_as_answer_choice_index": null, "total_logprob": null, "num_shared_ngrams": [0, 0, 0, 0, 0]}
