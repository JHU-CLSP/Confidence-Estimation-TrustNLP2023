{"id": "080ef6941410139d6869e78122bc741e", "untokenized_input": "A beaver is know for building prowess, their supplies come from where?\n- british columbia\n- body of water\n- wooded area\n- pay debts\n- zoo\n\nThe best answer is", "untokenized_target": "wooded area", "untokenized_answer_choices": ["british columbia", "body of water", "wooded area", "pay debts", "zoo"], "tokenized_input": [32, 307, 8770, 318, 760, 329, 2615, 30721, 11, 511, 9416, 1282, 422, 810, 30, 198, 12, 275, 799, 680, 951, 2178, 544, 198, 12, 1767, 286, 1660, 198, 12, 4898, 276, 1989, 198, 12, 1414, 20250, 198, 12, 26626, 198, 198, 464, 1266, 3280, 318], "tokenized_answer_choices": [[65, 799, 680, 951, 2178, 544], [2618, 286, 1660, 50256, 50256, 50256], [3822, 276, 1989, 50256, 50256, 50256], [15577, 20250, 50256, 50256, 50256, 50256], [89, 2238, 50256, 50256, 50256, 50256]], "target_index": 2, "api_call_parameters": {"model": "text-davinci-002", "prompt": "A beaver is know for building prowess, their supplies come from where?\n- british columbia\n- body of water\n- wooded area\n- pay debts\n- zoo\n\nThe best answer is\n\n", "stop": ["<|endoftext|>"], "temperature": 0, "logprobs": 1}, "gpt3_response": {"id": "cmpl-6JVpv6DJgnQMOhGs12FiajjhyJAPY", "object": "text_completion", "created": 1670108631, "model": "text-davinci-002", "choices": [{"text": "\n\na wooded area.", "index": 0, "logprobs": {"tokens": ["\n", "\n", "a", " wood", "ed", " area", ".", "<|endoftext|>"], "token_logprobs": [-0.0052567767, -0.052887276, -0.5726808, -0.07178469, -6.6619094e-07, -1.8310955e-05, -0.7204986, -0.06632128], "top_logprobs": [{"\n": -0.0052567767}, {"\n": -0.052887276}, {"a": -0.5726808}, {" wood": -0.07178469}, {"ed": -6.6619094e-07}, {" area": -1.8310955e-05}, {".": -0.7204986}, {"<|endoftext|>": -0.06632128}], "text_offset": [159, 160, 161, 162, 167, 169, 174, 175]}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 47, "completion_tokens": 7, "total_tokens": 54}}, "parsed_prediction": "wooded area", "parsed_prediction_is_correct": 1, "prediction_as_answer_choice": "wooded area", "prediction_as_answer_choice_index": 2, "total_logprob": -1.4894483998459398, "num_shared_ngrams": [0, 0, 3, 0, 0]}
{"id": "8ae24d3ff199077a59e0d970feb665b7", "untokenized_input": "A car was hailed to chauffeur someone to the opera house, where was it heading?\n- go downtown\n- appear suddenly\n- go fast\n- bottom out\n- east\n\nThe best answer is", "untokenized_target": "go downtown", "untokenized_answer_choices": ["go downtown", "appear suddenly", "go fast", "bottom out", "east"], "tokenized_input": [32, 1097, 373, 27052, 284, 46901, 16658, 333, 2130, 284, 262, 27296, 2156, 11, 810, 373, 340, 9087, 30, 198, 12, 467, 9436, 198, 12, 1656, 6451, 198, 12, 467, 3049, 198, 12, 4220, 503, 198, 12, 7627, 198, 198, 464, 1266, 3280, 318], "tokenized_answer_choices": [[2188, 9436, 50256], [1324, 451, 6451], [2188, 3049, 50256], [22487, 503, 50256], [23316, 50256, 50256]], "target_index": 0, "api_call_parameters": {"model": "text-davinci-002", "prompt": "A car was hailed to chauffeur someone to the opera house, where was it heading?\n- go downtown\n- appear suddenly\n- go fast\n- bottom out\n- east\n\nThe best answer is\n\n", "stop": ["<|endoftext|>"], "temperature": 0, "logprobs": 1}, "gpt3_response": {"id": "cmpl-6JVpvUZlQy2psW3TN3zTQP4keDyq3", "object": "text_completion", "created": 1670108631, "model": "text-davinci-002", "choices": [{"text": "\n\nThe car was heading to the opera house downtown.", "index": 0, "logprobs": {"tokens": ["\n", "\n", "The", " car", " was", " heading", " to", " the", " opera", " house", " downtown", ".", "<|endoftext|>", "Q", ":", " How"], "token_logprobs": [-0.00072828965, -0.019369205, -0.39892244, -0.027155578, -0.0058448757, -1.0281943, -0.4397351, -0.0015071799, -0.006288155, -2.5744682e-06, -0.030363465, -1.1873685e-05, -1.7236207e-05, -2.1325443, 0, -1.586067], "top_logprobs": [{"\n": -0.00072828965}, {"\n": -0.019369205}, {"The": -0.39892244}, {" car": -0.027155578}, {" was": -0.0058448757}, {" heading": -1.0281943}, {" to": -0.4397351}, {" the": -0.0015071799}, {" opera": -0.006288155}, {" house": -2.5744682e-06}, {" downtown": -0.030363465}, {".": -1.1873685e-05}, {"<|endoftext|>": -1.7236207e-05}, {"Q": -2.1325443}, {":": 0}, {" How": -1.586067}], "text_offset": [163, 164, 165, 168, 172, 176, 184, 187, 191, 197, 203, 212, 213, 213, 213, 213]}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 45, "completion_tokens": 12, "total_tokens": 57}}, "parsed_prediction": "go downtown", "parsed_prediction_is_correct": 1, "prediction_as_answer_choice": "go downtown", "prediction_as_answer_choice_index": 0, "total_logprob": -1.9581402726102, "num_shared_ngrams": [1, 0, 0, 0, 0]}
{"id": "9c784727afd7176b54764055df7a7927", "untokenized_input": "A child wants to play, what would they likely want?\n- fall down\n- breathe\n- play tag\n- be dismembered by a chainsaw\n- become adult\n\nThe best answer is", "untokenized_target": "play tag", "untokenized_answer_choices": ["fall down", "breathe", "play tag", "be dismembered by a chainsaw", "become adult"], "tokenized_input": [32, 1200, 3382, 284, 711, 11, 644, 561, 484, 1884, 765, 30, 198, 12, 2121, 866, 198, 12, 18044, 198, 12, 711, 7621, 198, 12, 307, 595, 11883, 9451, 416, 257, 14659, 707, 198, 12, 1716, 4044, 198, 198, 464, 1266, 3280, 318], "tokenized_answer_choices": [[7207, 866, 50256, 50256, 50256, 50256, 50256, 50256], [65, 630, 258, 50256, 50256, 50256, 50256, 50256], [1759, 7621, 50256, 50256, 50256, 50256, 50256, 50256], [1350, 595, 11883, 9451, 416, 257, 14659, 707], [9423, 462, 4044, 50256, 50256, 50256, 50256, 50256]], "target_index": 2, "api_call_parameters": {"model": "text-davinci-002", "prompt": "A child wants to play, what would they likely want?\n- fall down\n- breathe\n- play tag\n- be dismembered by a chainsaw\n- become adult\n\nThe best answer is\n\n", "stop": ["<|endoftext|>"], "temperature": 0, "logprobs": 1}, "gpt3_response": {"id": "cmpl-6JVpwIoQ4ifhxSs2Q0aAmSD6YO3rT", "object": "text_completion", "created": 1670108632, "model": "text-davinci-002", "choices": [{"text": "\n\nplay tag", "index": 0, "logprobs": {"tokens": ["\n", "\n", "play", " tag", "<|endoftext|>", "Q", ":", " How"], "token_logprobs": [-0.015900813, -0.23472632, -0.08965801, -0.0001101101, -0.0008895902, -1.9766473, 0, -1.5927846], "top_logprobs": [{"\n": -0.015900813}, {"\n": -0.23472632}, {"play": -0.08965801}, {" tag": -0.0001101101}, {"<|endoftext|>": -0.0008895902}, {"Q": -1.9766473}, {":": 0}, {" How": -1.5927846}], "text_offset": [152, 153, 154, 158, 162, 162, 162, 162]}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 44, "completion_tokens": 4, "total_tokens": 48}}, "parsed_prediction": "play tag", "parsed_prediction_is_correct": 1, "prediction_as_answer_choice": "play tag", "prediction_as_answer_choice_index": 2, "total_logprob": -0.34128484330000003, "num_shared_ngrams": [0, 0, 3, 0, 0]}
{"id": "a2aa95861ef74bf1ecfc55db505e3982", "untokenized_input": "A farmer sees a weasel in the woods, where is the farmer?\n- chicken coop\n- beach\n- fairytale\n- great outdoors\n- corn fields\n\nThe best answer is", "untokenized_target": "great outdoors", "untokenized_answer_choices": ["chicken coop", "beach", "fairytale", "great outdoors", "corn fields"], "tokenized_input": [32, 18739, 7224, 257, 356, 48038, 287, 262, 16479, 11, 810, 318, 262, 18739, 30, 198, 12, 9015, 763, 404, 198, 12, 10481, 198, 12, 3148, 20760, 1000, 198, 12, 1049, 24349, 198, 12, 11676, 7032, 198, 198, 464, 1266, 3280, 318], "tokenized_answer_choices": [[354, 5973, 763, 404], [1350, 620, 50256, 50256], [69, 13021, 29429, 50256], [18223, 24349, 50256, 50256], [20772, 7032, 50256, 50256]], "target_index": 3, "api_call_parameters": {"model": "text-davinci-002", "prompt": "A farmer sees a weasel in the woods, where is the farmer?\n- chicken coop\n- beach\n- fairytale\n- great outdoors\n- corn fields\n\nThe best answer is\n\n", "stop": ["<|endoftext|>"], "temperature": 0, "logprobs": 1}, "gpt3_response": {"id": "cmpl-6JVpxECwpKOceMQdDCE4liiCPriTx", "object": "text_completion", "created": 1670108633, "model": "text-davinci-002", "choices": [{"text": "\nThe farmer is in the great outdoors.", "index": 0, "logprobs": {"tokens": ["\n", "The", " farmer", " is", " in", " the", " great", " outdoors", ".", "<|endoftext|>", "#", "include", " <", "std", "io", "."], "token_logprobs": [-0.0029465028, -0.70979315, -0.014578261, -0.037295755, -0.014032156, -9.055464e-07, -0.34078187, -9.055464e-07, -0.0010876146, -4.763911e-05, -2.011765, -1.644449, -0.5890723, -1.1462052, -0.0975677, -0.000109035354], "top_logprobs": [{"\n": -0.0029465028}, {"The": -0.70979315}, {" farmer": -0.014578261}, {" is": -0.037295755}, {" in": -0.014032156}, {" the": -9.055464e-07}, {" great": -0.34078187}, {" outdoors": -9.055464e-07}, {".": -0.0010876146}, {"<|endoftext|>": -4.763911e-05}, {"#": -2.011765}, {"include": -1.644449}, {" <": -0.5890723}, {"std": -1.1462052}, {"io": -0.0975677}, {".": -0.000109035354}], "text_offset": [145, 146, 149, 156, 159, 162, 166, 172, 181, 182, 182, 182, 182, 182, 182, 182]}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 43, "completion_tokens": 9, "total_tokens": 52}}, "parsed_prediction": "great outdoors", "parsed_prediction_is_correct": 1, "prediction_as_answer_choice": "great outdoors", "prediction_as_answer_choice_index": 3, "total_logprob": -1.1205647596028, "num_shared_ngrams": [0, 0, 0, 3, 0]}
{"id": "d107d67d525a686fbd8282314d2ea33c", "untokenized_input": "A gentleman is carrying equipment for golf, what is he likely to have?\n- club\n- assembly hall\n- meditation center\n- meeting\n- church\n\nThe best answer is", "untokenized_target": "club", "untokenized_answer_choices": ["club", "assembly hall", "meditation center", "meeting", "church"], "tokenized_input": [32, 22427, 318, 6872, 5112, 329, 13126, 11, 644, 318, 339, 1884, 284, 423, 30, 198, 12, 3430, 198, 12, 10474, 6899, 198, 12, 16901, 3641, 198, 12, 3249, 198, 12, 4928, 198, 198, 464, 1266, 3280, 318], "tokenized_answer_choices": [[18664, 50256, 50256], [41873, 6899, 50256], [1150, 3780, 3641], [1326, 13629, 50256], [36964, 50256, 50256]], "target_index": 0, "api_call_parameters": {"model": "text-davinci-002", "prompt": "A gentleman is carrying equipment for golf, what is he likely to have?\n- club\n- assembly hall\n- meditation center\n- meeting\n- church\n\nThe best answer is\n\n", "stop": ["<|endoftext|>"], "temperature": 0, "logprobs": 1}, "gpt3_response": {"id": "cmpl-6JVpyjhIQQQ4z99EPncvJ2Sc8b3cL", "object": "text_completion", "created": 1670108634, "model": "text-davinci-002", "choices": [{"text": "\nclub", "index": 0, "logprobs": {"tokens": ["\n", "club", "<|endoftext|>", "Q", ":", " How", " to", " get"], "token_logprobs": [-0.028442921, -1.1296833, -0.015092592, -0.36812058, 0, -1.6032344, -0.47295678, -2.7259266], "top_logprobs": [{"\n": -0.028442921}, {"club": -1.1296833}, {"<|endoftext|>": -0.015092592}, {"Q": -0.36812058}, {":": 0}, {" How": -1.6032344}, {" to": -0.47295678}, {" get": -2.7259266}], "text_offset": [154, 155, 159, 159, 159, 159, 159, 159]}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 39, "completion_tokens": 2, "total_tokens": 41}}, "parsed_prediction": "club", "parsed_prediction_is_correct": 1, "prediction_as_answer_choice": "club", "prediction_as_answer_choice_index": 0, "total_logprob": -1.1732188129999999, "num_shared_ngrams": [1, 0, 0, 0, 0]}
{"id": "dc55d473c22b04877b11d584f9548194", "untokenized_input": "A human wants to submerge himself in water, what should he use?\n- whirlpool bath\n- coffee cup\n- cup\n- soft drink\n- puddle\n\nThe best answer is", "untokenized_target": "whirlpool bath", "untokenized_answer_choices": ["whirlpool bath", "coffee cup", "cup", "soft drink", "puddle"], "tokenized_input": [32, 1692, 3382, 284, 32640, 469, 2241, 287, 1660, 11, 644, 815, 339, 779, 30, 198, 12, 348, 1901, 7742, 7837, 198, 12, 6891, 6508, 198, 12, 6508, 198, 12, 2705, 4144, 198, 12, 279, 24500, 198, 198, 464, 1266, 3280, 318], "tokenized_answer_choices": [[1929, 1901, 7742, 7837], [1073, 5853, 6508, 50256], [25244, 50256, 50256, 50256], [4215, 4144, 50256, 50256], [79, 24500, 50256, 50256]], "target_index": 0, "api_call_parameters": {"model": "text-davinci-002", "prompt": "A human wants to submerge himself in water, what should he use?\n- whirlpool bath\n- coffee cup\n- cup\n- soft drink\n- puddle\n\nThe best answer is\n\n", "stop": ["<|endoftext|>"], "temperature": 0, "logprobs": 1}, "gpt3_response": {"id": "cmpl-6JVpyyJRUPSnrH1avWZqCOqx6UiEB", "object": "text_completion", "created": 1670108634, "model": "text-davinci-002", "choices": [{"text": "\n\na whirlpool bath.", "index": 0, "logprobs": {"tokens": ["\n", "\n", "a", " wh", "irl", "pool", " bath", ".", "<|endoftext|>", "#", "include", " <", "std", "io", ".", "h"], "token_logprobs": [-0.00016757715, -0.46736437, -0.7934709, -0.106168754, -2.2159093e-06, -6.827632e-08, -1.1430258e-06, -0.040757585, -0.012007782, -1.8312819, -1.4346524, -0.6251023, -1.1637118, -0.10665651, -0.00010176362, -1.9765537e-06], "top_logprobs": [{"\n": -0.00016757715}, {"\n": -0.46736437}, {"a": -0.7934709}, {" wh": -0.106168754}, {"irl": -2.2159093e-06}, {"pool": -6.827632e-08}, {" bath": -1.1430258e-06}, {".": -0.040757585}, {"<|endoftext|>": -0.012007782}, {"#": -1.8312819}, {"include": -1.4346524}, {" <": -0.6251023}, {"std": -1.1637118}, {"io": -0.10665651}, {".": -0.00010176362}, {"h": -1.9765537e-06}], "text_offset": [143, 144, 145, 146, 149, 152, 156, 161, 162, 162, 162, 162, 162, 162, 162, 162]}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 43, "completion_tokens": 8, "total_tokens": 51}}, "parsed_prediction": "whirlpool bath", "parsed_prediction_is_correct": 1, "prediction_as_answer_choice": "whirlpool bath", "prediction_as_answer_choice_index": 0, "total_logprob": -1.4199403953614198, "num_shared_ngrams": [3, 0, 0, 0, 0]}
{"id": "43ab0ff711e60d51f943bbd2cdd6515a", "untokenized_input": "A loud machine is irritating, but many are expected where?\n- museum\n- house\n- laboratory\n- library\n- industrial area\n\nThe best answer is", "untokenized_target": "industrial area", "untokenized_answer_choices": ["museum", "house", "laboratory", "library", "industrial area"], "tokenized_input": [32, 7812, 4572, 318, 42010, 11, 475, 867, 389, 2938, 810, 30, 198, 12, 13257, 198, 12, 2156, 198, 12, 14010, 198, 12, 5888, 198, 12, 7593, 1989, 198, 198, 464, 1266, 3280, 318], "tokenized_answer_choices": [[76, 6744, 50256], [4803, 50256, 50256], [75, 4820, 2870], [32016, 50256, 50256], [31130, 1989, 50256]], "target_index": 4, "api_call_parameters": {"model": "text-davinci-002", "prompt": "A loud machine is irritating, but many are expected where?\n- museum\n- house\n- laboratory\n- library\n- industrial area\n\nThe best answer is\n\n", "stop": ["<|endoftext|>"], "temperature": 0, "logprobs": 1}, "gpt3_response": {"id": "cmpl-6JVpztxtQpBMJsnH0RD4qprjndchh", "object": "text_completion", "created": 1670108635, "model": "text-davinci-002", "choices": [{"text": "\n\nindustrial area", "index": 0, "logprobs": {"tokens": ["\n", "\n", "industrial", " area", "<|endoftext|>", "Q", ":", " How"], "token_logprobs": [-0.0008054749, -0.14660752, -0.032999795, -3.643162e-05, -3.7028603e-05, -0.9298882, 0, -1.6123588], "top_logprobs": [{"\n": -0.0008054749}, {"\n": -0.14660752}, {"industrial": -0.032999795}, {" area": -3.643162e-05}, {"<|endoftext|>": -3.7028603e-05}, {"Q": -0.9298882}, {":": 0}, {" How": -1.6123588}], "text_offset": [138, 139, 140, 150, 155, 155, 155, 155]}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 35, "completion_tokens": 4, "total_tokens": 39}}, "parsed_prediction": "industrial area", "parsed_prediction_is_correct": 1, "prediction_as_answer_choice": "industrial area", "prediction_as_answer_choice_index": 4, "total_logprob": -0.18048625012299999, "num_shared_ngrams": [0, 0, 0, 0, 3]}
{"id": "8e1b0792e441a5d54ae47a4b24f48977", "untokenized_input": "A man takes a seat at a museum outside of Barcelona, where is he likely?\n- in cinema\n- martorell\n- falling down\n- show\n- airplane\n\nThe best answer is", "untokenized_target": "martorell", "untokenized_answer_choices": ["in cinema", "martorell", "falling down", "show", "airplane"], "tokenized_input": [32, 582, 2753, 257, 5852, 379, 257, 13257, 2354, 286, 15142, 11, 810, 318, 339, 1884, 30, 198, 12, 287, 22041, 198, 12, 11277, 382, 297, 198, 12, 7463, 866, 198, 12, 905, 198, 12, 19401, 198, 198, 464, 1266, 3280, 318], "tokenized_answer_choices": [[259, 22041, 50256], [13822, 382, 297], [7207, 278, 866], [12860, 50256, 50256], [958, 14382, 50256]], "target_index": 1, "api_call_parameters": {"model": "text-davinci-002", "prompt": "A man takes a seat at a museum outside of Barcelona, where is he likely?\n- in cinema\n- martorell\n- falling down\n- show\n- airplane\n\nThe best answer is\n\n", "stop": ["<|endoftext|>"], "temperature": 0, "logprobs": 1}, "gpt3_response": {"id": "cmpl-6JVpzo1NdSt3fKO58IJHi9DWQbIk3", "object": "text_completion", "created": 1670108635, "model": "text-davinci-002", "choices": [{"text": "\n\nThe man is likely in Barcelona, Spain.", "index": 0, "logprobs": {"tokens": ["\n", "\n", "The", " man", " is", " likely", " in", " Barcelona", ",", " Spain", ".", "<|endoftext|>", "#", "include", " <", "std"], "token_logprobs": [-0.00028925636, -0.307369, -0.5934714, -0.004282602, -0.00057488505, -0.017513543, -0.28975537, -0.05192756, -0.31140953, -0.32693192, -0.0007134374, -2.2159093e-06, -2.0625606, -1.8935207, -0.5791914, -1.1650552], "top_logprobs": [{"\n": -0.00028925636}, {"\n": -0.307369}, {"The": -0.5934714}, {" man": -0.004282602}, {" is": -0.00057488505}, {" likely": -0.017513543}, {" in": -0.28975537}, {" Barcelona": -0.05192756}, {",": -0.31140953}, {" Spain": -0.32693192}, {".": -0.0007134374}, {"<|endoftext|>": -2.2159093e-06}, {"#": -2.0625606}, {"include": -1.8935207}, {" <": -0.5791914}, {"std": -1.1650552}], "text_offset": [151, 152, 153, 156, 160, 163, 170, 173, 183, 184, 190, 191, 191, 191, 191, 191]}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 43, "completion_tokens": 11, "total_tokens": 54}}, "parsed_prediction": "in cinema", "parsed_prediction_is_correct": 0, "prediction_as_answer_choice": "in cinema", "prediction_as_answer_choice_index": 0, "total_logprob": -1.9042407197192999, "num_shared_ngrams": [1, 0, 0, 0, 0]}
{"id": "4da33e6f4b789776acb1bc10195baa83", "untokenized_input": "A man wants air conditioning while we watches the game on Saturday, where will it likely be installed?\n- car\n- house\n- offices\n- park\n- movie theatre\n\nThe best answer is", "untokenized_target": "house", "untokenized_answer_choices": ["car", "house", "offices", "park", "movie theatre"], "tokenized_input": [32, 582, 3382, 1633, 21143, 981, 356, 16860, 262, 983, 319, 3909, 11, 810, 481, 340, 1884, 307, 6589, 30, 198, 12, 1097, 198, 12, 2156, 198, 12, 9730, 198, 12, 3952, 198, 12, 3807, 21421, 198, 198, 464, 1266, 3280, 318], "tokenized_answer_choices": [[7718, 50256], [4803, 50256], [2364, 1063], [20928, 50256], [41364, 21421]], "target_index": 1, "api_call_parameters": {"model": "text-davinci-002", "prompt": "A man wants air conditioning while we watches the game on Saturday, where will it likely be installed?\n- car\n- house\n- offices\n- park\n- movie theatre\n\nThe best answer is\n\n", "stop": ["<|endoftext|>"], "temperature": 0, "logprobs": 1}, "gpt3_response": {"id": "cmpl-6JVq0Xdd6heIg9o8hiWE64sRT93Jt", "object": "text_completion", "created": 1670108636, "model": "text-davinci-002", "choices": [{"text": "\n\nmovie theatre", "index": 0, "logprobs": {"tokens": ["\n", "\n", "movie", " theatre", "<|endoftext|>", "Q", ":", " How"], "token_logprobs": [-0.005816816, -0.92938054, -0.80403847, -0.0002311867, -0.00036992683, -1.2639532, 0, -1.6018732], "top_logprobs": [{"\n": -0.005816816}, {"\n": -0.92938054}, {"movie": -0.80403847}, {" theatre": -0.0002311867}, {"<|endoftext|>": -0.00036992683}, {"Q": -1.2639532}, {":": 0}, {" How": -1.6018732}], "text_offset": [171, 172, 173, 178, 186, 186, 186, 186]}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 43, "completion_tokens": 4, "total_tokens": 47}}, "parsed_prediction": "movie theatre", "parsed_prediction_is_correct": 0, "prediction_as_answer_choice": "movie theatre", "prediction_as_answer_choice_index": 4, "total_logprob": -1.7398369395299997, "num_shared_ngrams": [0, 0, 0, 0, 3]}
{"id": "3a3b5d4a517ef70d25eb558f1a622937", "untokenized_input": "A patriotic guy with a camera is looking for a bald eagle, what is he likely to do with the eagle if he finds one?\n- city\n- canada\n- minnesota\n- thermal\n- photograph\n\nThe best answer is", "untokenized_target": "photograph", "untokenized_answer_choices": ["city", "canada", "minnesota", "thermal", "photograph"], "tokenized_input": [32, 33453, 3516, 351, 257, 4676, 318, 2045, 329, 257, 28273, 31176, 11, 644, 318, 339, 1884, 284, 466, 351, 262, 31176, 611, 339, 7228, 530, 30, 198, 12, 1748, 198, 12, 460, 4763, 198, 12, 949, 8360, 198, 12, 18411, 198, 12, 8408, 198, 198, 464, 1266, 3280, 318], "tokenized_answer_choices": [[19205, 50256], [5171, 4763], [1084, 8360], [490, 7617], [38611, 2384]], "target_index": 4, "api_call_parameters": {"model": "text-davinci-002", "prompt": "A patriotic guy with a camera is looking for a bald eagle, what is he likely to do with the eagle if he finds one?\n- city\n- canada\n- minnesota\n- thermal\n- photograph\n\nThe best answer is\n\n", "stop": ["<|endoftext|>"], "temperature": 0, "logprobs": 1}, "gpt3_response": {"id": "cmpl-6JVq0HM5djx4ciEOYNPJTHTilIxT6", "object": "text_completion", "created": 1670108636, "model": "text-davinci-002", "choices": [{"text": "\n\nHe is likely to photograph the eagle.", "index": 0, "logprobs": {"tokens": ["\n", "\n", "He", " is", " likely", " to", " photograph", " the", " eagle", ".", "<|endoftext|>", "#", "include", " <", "std", "io"], "token_logprobs": [-0.007723074, -0.28225982, -1.1649771, -0.10806571, -0.014630761, -0.0014409816, -0.35117793, -0.16158363, -0.3650729, -0.11105111, -1.402038e-05, -2.2817638, -1.6790057, -0.5494278, -1.1195272, -0.090848275], "top_logprobs": [{"\n": -0.007723074}, {"\n": -0.28225982}, {"He": -1.1649771}, {" is": -0.10806571}, {" likely": -0.014630761}, {" to": -0.0014409816}, {" photograph": -0.35117793}, {" the": -0.16158363}, {" eagle": -0.3650729}, {".": -0.11105111}, {"<|endoftext|>": -1.402038e-05}, {"#": -2.2817638}, {"include": -1.6790057}, {" <": -0.5494278}, {"std": -1.1195272}, {"io": -0.090848275}], "text_offset": [187, 188, 189, 191, 194, 201, 204, 215, 219, 225, 226, 226, 226, 226, 226, 226]}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 51, "completion_tokens": 10, "total_tokens": 61}}, "parsed_prediction": "photograph", "parsed_prediction_is_correct": 1, "prediction_as_answer_choice": "photograph", "prediction_as_answer_choice_index": 4, "total_logprob": -2.56799703698, "num_shared_ngrams": [0, 0, 0, 0, 1]}
