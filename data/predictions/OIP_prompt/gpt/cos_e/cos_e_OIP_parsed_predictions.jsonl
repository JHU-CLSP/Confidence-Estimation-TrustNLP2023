{"id": "080ef6941410139d6869e78122bc741e", "question": "A beaver is know for building prowess, their supplies come from where?", "choices": ["british columbia", "body of water", "wooded area", "pay debts", "zoo"], "answer": "wooded area", "answer_index": 2, "prompt": "A beaver is know for building prowess, their supplies come from where?\nbritish columbia, body of water, wooded area, pay debts, or zoo", "api_call_parameters": {"model": "text-davinci-002", "prompt": "A beaver is know for building prowess, their supplies come from where?\nbritish columbia, body of water, wooded area, pay debts, or zoo\n\n", "stop": ["<|endoftext|>"], "temperature": 0, "logprobs": 1}, "gpt3_response": {"id": "cmpl-6SBoRsvUhna7MuueD3QmcRiSTt9fB", "object": "text_completion", "created": 1672176611, "model": "text-davinci-002", "choices": [{"text": "\n\nA beaver's supplies come from a wooded area.", "index": 0, "logprobs": {"tokens": ["\n", "\n", "A", " be", "aver", "'s", " supplies", " come", " from", " a", " wood", "ed", " area", ".", "<|endoftext|>", "Q"], "token_logprobs": [-0.017794427, -0.031009046, -0.4238892, -0.004228431, -0.0009212678, -0.03515469, -0.22599214, -0.006967785, -0.009832119, -1.1016746, -0.9261779, -1.8841804e-07, -1.8788729e-05, -0.035551, -0.0003739799, -1.0894833], "top_logprobs": [{"\n": -0.017794427}, {"\n": -0.031009046}, {"A": -0.4238892}, {" be": -0.004228431}, {"aver": -0.0009212678}, {"'s": -0.03515469}, {" supplies": -0.22599214}, {" come": -0.006967785}, {" from": -0.009832119}, {" a": -1.1016746}, {" wood": -0.9261779}, {"ed": -1.8841804e-07}, {" area": -1.8788729e-05}, {".": -0.035551}, {"<|endoftext|>": -0.0003739799}, {"Q": -1.0894833}], "text_offset": [136, 137, 138, 139, 142, 146, 148, 157, 162, 167, 169, 174, 176, 181, 182, 182]}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 37, "completion_tokens": 14, "total_tokens": 51}}, "parsed_prediction": "wooded area", "parsed_prediction_is_correct": 1, "parsed_prediction_response_type": "text", "total_logprob": -3.9090688628470396, "prediction_as_answer_choice": "wooded area", "prediction_as_answer_choice_index": 2, "num_shared_ngrams": [0, 0, 3, 0, 0]}
{"id": "8ae24d3ff199077a59e0d970feb665b7", "question": "A car was hailed to chauffeur someone to the opera house, where was it heading?", "choices": ["go downtown", "appear suddenly", "go fast", "bottom out", "east"], "answer": "go downtown", "answer_index": 0, "prompt": "A car was hailed to chauffeur someone to the opera house, where was it heading?\ngo downtown, appear suddenly, go fast, bottom out, or east", "api_call_parameters": {"model": "text-davinci-002", "prompt": "A car was hailed to chauffeur someone to the opera house, where was it heading?\ngo downtown, appear suddenly, go fast, bottom out, or east\n\n", "stop": ["<|endoftext|>"], "temperature": 0, "logprobs": 1}, "gpt3_response": {"id": "cmpl-6SBoRTv0ycKIuUIM39uMDt4pWgNvx", "object": "text_completion", "created": 1672176611, "model": "text-davinci-002", "choices": [{"text": "\nThe car was heading to the opera house downtown.", "index": 0, "logprobs": {"tokens": ["\n", "The", " car", " was", " heading", " to", " the", " opera", " house", " downtown", ".", "<|endoftext|>", "Q", ":", " How"], "token_logprobs": [-0.00024751923, -0.42154235, -0.028247623, -0.028685978, -0.9079519, -0.8640968, -0.0037621902, -0.0035793663, -4.277735e-07, -0.22000091, -1.4974988e-05, -0.00053588126, -1.9255279, -1.8841804e-07, -1.5759714], "top_logprobs": [{"\n": -0.00024751923}, {"The": -0.42154235}, {" car": -0.028247623}, {" was": -0.028685978}, {" heading": -0.9079519}, {" to": -0.8640968}, {" the": -0.0037621902}, {" opera": -0.0035793663}, {" house": -4.277735e-07}, {" downtown": -0.22000091}, {".": -1.4974988e-05}, {"<|endoftext|>": -0.00053588126}, {"Q": -1.9255279}, {":": -1.8841804e-07}, {" How": -1.5759714}], "text_offset": [140, 141, 144, 148, 152, 160, 163, 167, 173, 179, 188, 189, 189, 189, 189]}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 35, "completion_tokens": 11, "total_tokens": 46}}, "parsed_prediction": "go downtown", "parsed_prediction_is_correct": 1, "parsed_prediction_response_type": "text", "total_logprob": -5.98016540916954, "prediction_as_answer_choice": "go downtown", "prediction_as_answer_choice_index": 0, "num_shared_ngrams": [1, 0, 0, 0, 0]}
{"id": "9c784727afd7176b54764055df7a7927", "question": "A child wants to play, what would they likely want?", "choices": ["fall down", "breathe", "play tag", "be dismembered by a chainsaw", "become adult"], "answer": "play tag", "answer_index": 2, "prompt": "A child wants to play, what would they likely want?\nfall down, breathe, play tag, be dismembered by a chainsaw, or become adult", "api_call_parameters": {"model": "text-davinci-002", "prompt": "A child wants to play, what would they likely want?\nfall down, breathe, play tag, be dismembered by a chainsaw, or become adult\n\n", "stop": ["<|endoftext|>"], "temperature": 0, "logprobs": 1}, "gpt3_response": {"id": "cmpl-6SBoRfqtYLMjU1TuvxoK1fKfqtPkv", "object": "text_completion", "created": 1672176611, "model": "text-davinci-002", "choices": [{"text": "\n\nA child would likely want to play tag or fall down.", "index": 0, "logprobs": {"tokens": ["\n", "\n", "A", " child", " would", " likely", " want", " to", " play", " tag", " or", " fall", " down", ".", "<|endoftext|>"], "token_logprobs": [-0.00095438055, -0.7447573, -0.67820334, -0.00096864055, -0.7519461, -0.12548846, -0.00043426573, -2.3912831e-05, -0.020888343, -0.14135095, -0.4230797, -0.4095273, -3.883893e-06, -3.6482897e-06, -0.00072142907], "top_logprobs": [{"\n": -0.00095438055}, {"\n": -0.7447573}, {"A": -0.67820334}, {" child": -0.00096864055}, {" would": -0.7519461}, {" likely": -0.12548846}, {" want": -0.00043426573}, {" to": -2.3912831e-05}, {" play": -0.020888343}, {" tag": -0.14135095}, {" or": -0.4230797}, {" fall": -0.4095273}, {" down": -3.883893e-06}, {".": -3.6482897e-06}, {"<|endoftext|>": -0.00072142907}], "text_offset": [129, 130, 131, 132, 138, 144, 151, 156, 159, 164, 168, 171, 176, 181, 182]}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 34, "completion_tokens": 14, "total_tokens": 48}}, "parsed_prediction": "fall down", "parsed_prediction_is_correct": 0, "parsed_prediction_response_type": "text", "total_logprob": -3.2983516539137008, "prediction_as_answer_choice": "fall down", "prediction_as_answer_choice_index": 0, "num_shared_ngrams": [3, 0, 3, 1, 0]}
{"id": "a2aa95861ef74bf1ecfc55db505e3982", "question": "A farmer sees a weasel in the woods, where is the farmer?", "choices": ["chicken coop", "beach", "fairytale", "great outdoors", "corn fields"], "answer": "great outdoors", "answer_index": 3, "prompt": "A farmer sees a weasel in the woods, where is the farmer?\nchicken coop, beach, fairytale, great outdoors, or corn fields", "api_call_parameters": {"model": "text-davinci-002", "prompt": "A farmer sees a weasel in the woods, where is the farmer?\nchicken coop, beach, fairytale, great outdoors, or corn fields\n\n", "stop": ["<|endoftext|>"], "temperature": 0, "logprobs": 1}, "gpt3_response": {"id": "cmpl-6SBoSYmAHI5ZGV0ykK2a4Be9YqZjD", "object": "text_completion", "created": 1672176612, "model": "text-davinci-002", "choices": [{"text": "\n\nThe farmer is in the woods.", "index": 0, "logprobs": {"tokens": ["\n", "\n", "The", " farmer", " is", " in", " the", " woods", ".", "<|endoftext|>", "#", "include", " <", "std"], "token_logprobs": [-0.003120182, -0.17943309, -0.19936267, -0.016787734, -0.055530686, -0.024259398, -7.75605e-05, -1.0016491, -0.0035335473, -4.859278e-05, -2.6750934, -1.1248931, -0.5264822, -1.1107332], "top_logprobs": [{"\n": -0.003120182}, {"\n": -0.17943309}, {"The": -0.19936267}, {" farmer": -0.016787734}, {" is": -0.055530686}, {" in": -0.024259398}, {" the": -7.75605e-05}, {" woods": -1.0016491}, {".": -0.0035335473}, {"<|endoftext|>": -4.859278e-05}, {"#": -2.6750934}, {"include": -1.1248931}, {" <": -0.5264822}, {"std": -1.1107332}], "text_offset": [122, 123, 124, 127, 134, 137, 140, 144, 150, 151, 151, 151, 151, 151]}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 34, "completion_tokens": 9, "total_tokens": 43}}, "parsed_prediction": null, "parsed_prediction_is_correct": null, "parsed_prediction_response_type": null, "total_logprob": null, "prediction_as_answer_choice": null, "prediction_as_answer_choice_index": null, "num_shared_ngrams": [0, 0, 0, 0, 0]}
{"id": "d107d67d525a686fbd8282314d2ea33c", "question": "A gentleman is carrying equipment for golf, what is he likely to have?", "choices": ["club", "assembly hall", "meditation center", "meeting", "church"], "answer": "club", "answer_index": 0, "prompt": "A gentleman is carrying equipment for golf, what is he likely to have?\nclub, assembly hall, meditation center, meeting, or church", "api_call_parameters": {"model": "text-davinci-002", "prompt": "A gentleman is carrying equipment for golf, what is he likely to have?\nclub, assembly hall, meditation center, meeting, or church\n\n", "stop": ["<|endoftext|>"], "temperature": 0, "logprobs": 1}, "gpt3_response": {"id": "cmpl-6SBoSjf7I2igdCKmw5PLWvMmKDe4e", "object": "text_completion", "created": 1672176612, "model": "text-davinci-002", "choices": [{"text": "\n\nA gentleman is likely to be carrying equipment for golf if he has a", "index": 0, "logprobs": {"tokens": ["\n", "\n", "A", " gentleman", " is", " likely", " to", " be", " carrying", " equipment", " for", " golf", " if", " he", " has", " a"], "token_logprobs": [-0.002637185, -0.44462284, -0.37624708, -0.1621256, -0.14831643, -0.4286785, -0.03578677, -0.47795594, -4.7399757e-05, -0.80827564, -4.1232483e-06, -0.0003095256, -0.0017822307, -6.827632e-08, -0.011426779, -7.517727e-05], "top_logprobs": [{"\n": -0.002637185}, {"\n": -0.44462284}, {"A": -0.37624708}, {" gentleman": -0.1621256}, {" is": -0.14831643}, {" likely": -0.4286785}, {" to": -0.03578677}, {" be": -0.47795594}, {" carrying": -4.7399757e-05}, {" equipment": -0.80827564}, {" for": -4.1232483e-06}, {" golf": -0.0003095256}, {" if": -0.0017822307}, {" he": -6.827632e-08}, {" has": -0.011426779}, {" a": -7.517727e-05}], "text_offset": [131, 132, 133, 134, 144, 147, 154, 157, 160, 169, 179, 183, 188, 191, 194, 198]}, "finish_reason": "length"}], "usage": {"prompt_tokens": 29, "completion_tokens": 16, "total_tokens": 45}}, "parsed_prediction": null, "parsed_prediction_is_correct": null, "parsed_prediction_response_type": null, "total_logprob": null, "prediction_as_answer_choice": null, "prediction_as_answer_choice_index": null, "num_shared_ngrams": [0, 0, 0, 0, 0]}
{"id": "dc55d473c22b04877b11d584f9548194", "question": "A human wants to submerge himself in water, what should he use?", "choices": ["whirlpool bath", "coffee cup", "cup", "soft drink", "puddle"], "answer": "whirlpool bath", "answer_index": 0, "prompt": "A human wants to submerge himself in water, what should he use?\nwhirlpool bath, coffee cup, cup, soft drink, or puddle", "api_call_parameters": {"model": "text-davinci-002", "prompt": "A human wants to submerge himself in water, what should he use?\nwhirlpool bath, coffee cup, cup, soft drink, or puddle\n\n", "stop": ["<|endoftext|>"], "temperature": 0, "logprobs": 1}, "gpt3_response": {"id": "cmpl-6SBoTIllBKk2RekwI6wwpVFImMyjB", "object": "text_completion", "created": 1672176613, "model": "text-davinci-002", "choices": [{"text": "\n\nA whirlpool bath would be the best option for a human to", "index": 0, "logprobs": {"tokens": ["\n", "\n", "A", " wh", "irl", "pool", " bath", " would", " be", " the", " best", " option", " for", " a", " human", " to"], "token_logprobs": [-0.0019410775, -0.16719079, -0.33450425, -0.56952876, -1.5805701e-05, -1.8841804e-07, -2.693682e-06, -0.13488655, -0.0067109875, -0.013966212, -0.0030678674, -0.04115426, -0.00038745708, -0.148276, -0.021156907, -0.02463864], "top_logprobs": [{"\n": -0.0019410775}, {"\n": -0.16719079}, {"A": -0.33450425}, {" wh": -0.56952876}, {"irl": -1.5805701e-05}, {"pool": -1.8841804e-07}, {" bath": -2.693682e-06}, {" would": -0.13488655}, {" be": -0.0067109875}, {" the": -0.013966212}, {" best": -0.0030678674}, {" option": -0.04115426}, {" for": -0.00038745708}, {" a": -0.148276}, {" human": -0.021156907}, {" to": -0.02463864}], "text_offset": [120, 121, 122, 123, 126, 129, 133, 138, 144, 147, 151, 156, 163, 167, 169, 175]}, "finish_reason": "length"}], "usage": {"prompt_tokens": 33, "completion_tokens": 16, "total_tokens": 49}}, "parsed_prediction": "whirlpool bath", "parsed_prediction_is_correct": 1, "parsed_prediction_response_type": "text", "total_logprob": -1.46742844628104, "prediction_as_answer_choice": "whirlpool bath", "prediction_as_answer_choice_index": 0, "num_shared_ngrams": [3, 0, 0, 0, 0]}
{"id": "43ab0ff711e60d51f943bbd2cdd6515a", "question": "A loud machine is irritating, but many are expected where?", "choices": ["museum", "house", "laboratory", "library", "industrial area"], "answer": "industrial area", "answer_index": 4, "prompt": "A loud machine is irritating, but many are expected where?\nmuseum, house, laboratory, library, or industrial area", "api_call_parameters": {"model": "text-davinci-002", "prompt": "A loud machine is irritating, but many are expected where?\nmuseum, house, laboratory, library, or industrial area\n\n", "stop": ["<|endoftext|>"], "temperature": 0, "logprobs": 1}, "gpt3_response": {"id": "cmpl-6SBoTZYaAOWUXUHuSJhGNRq49BC8i", "object": "text_completion", "created": 1672176613, "model": "text-davinci-002", "choices": [{"text": "\n\nA loud machine is irritating, but many are expected in an industrial area", "index": 0, "logprobs": {"tokens": ["\n", "\n", "A", " loud", " machine", " is", " irritating", ",", " but", " many", " are", " expected", " in", " an", " industrial", " area"], "token_logprobs": [-0.0037163014, -0.29397437, -0.52203375, -0.30722585, -0.000102479804, -0.19185701, -0.21622828, -0.001021801, -0.0003989058, -0.00038853093, -0.0018577697, -0.00024597047, -0.38396162, -0.014962441, -0.0012887204, -2.6419024e-05], "top_logprobs": [{"\n": -0.0037163014}, {"\n": -0.29397437}, {"A": -0.52203375}, {" loud": -0.30722585}, {" machine": -0.000102479804}, {" is": -0.19185701}, {" irritating": -0.21622828}, {",": -0.001021801}, {" but": -0.0003989058}, {" many": -0.00038853093}, {" are": -0.0018577697}, {" expected": -0.00024597047}, {" in": -0.38396162}, {" an": -0.014962441}, {" industrial": -0.0012887204}, {" area": -2.6419024e-05}], "text_offset": [115, 116, 117, 118, 123, 131, 134, 145, 146, 150, 155, 159, 168, 171, 174, 185]}, "finish_reason": "length"}], "usage": {"prompt_tokens": 26, "completion_tokens": 16, "total_tokens": 42}}, "parsed_prediction": "industrial area", "parsed_prediction_is_correct": 1, "parsed_prediction_response_type": "text", "total_logprob": -1.939290219528, "prediction_as_answer_choice": "industrial area", "prediction_as_answer_choice_index": 4, "num_shared_ngrams": [0, 0, 0, 0, 3]}
{"id": "8e1b0792e441a5d54ae47a4b24f48977", "question": "A man takes a seat at a museum outside of Barcelona, where is he likely?", "choices": ["in cinema", "martorell", "falling down", "show", "airplane"], "answer": "martorell", "answer_index": 1, "prompt": "A man takes a seat at a museum outside of Barcelona, where is he likely?\nin cinema, martorell, falling down, show, or airplane", "api_call_parameters": {"model": "text-davinci-002", "prompt": "A man takes a seat at a museum outside of Barcelona, where is he likely?\nin cinema, martorell, falling down, show, or airplane\n\n", "stop": ["<|endoftext|>"], "temperature": 0, "logprobs": 1}, "gpt3_response": {"id": "cmpl-6SBoTUhAoeWi9jTJwdfKQ1UWEChpG", "object": "text_completion", "created": 1672176613, "model": "text-davinci-002", "choices": [{"text": "\n\nThe man is likely in Barcelona.", "index": 0, "logprobs": {"tokens": ["\n", "\n", "The", " man", " is", " likely", " in", " Barcelona", ".", "<|endoftext|>", "Q", ":", " How"], "token_logprobs": [-0.008617791, -0.11812242, -0.41659647, -0.010361181, -0.0015618019, -0.021017928, -0.11993121, -0.22730525, -0.15132007, -6.827632e-08, -1.2744485, -7.8540467e-07, -1.5442312], "top_logprobs": [{"\n": -0.008617791}, {"\n": -0.11812242}, {"The": -0.41659647}, {" man": -0.010361181}, {" is": -0.0015618019}, {" likely": -0.021017928}, {" in": -0.11993121}, {" Barcelona": -0.22730525}, {".": -0.15132007}, {"<|endoftext|>": -6.827632e-08}, {"Q": -1.2744485}, {":": -7.8540467e-07}, {" How": -1.5442312}], "text_offset": [128, 129, 130, 133, 137, 140, 147, 150, 160, 161, 161, 161, 161]}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 33, "completion_tokens": 9, "total_tokens": 42}}, "parsed_prediction": "in cinema", "parsed_prediction_is_correct": 0, "parsed_prediction_response_type": "text", "total_logprob": -3.8935146755809904, "prediction_as_answer_choice": "in cinema", "prediction_as_answer_choice_index": 0, "num_shared_ngrams": [1, 0, 0, 0, 0]}
{"id": "4da33e6f4b789776acb1bc10195baa83", "question": "A man wants air conditioning while we watches the game on Saturday, where will it likely be installed?", "choices": ["car", "house", "offices", "park", "movie theatre"], "answer": "house", "answer_index": 1, "prompt": "A man wants air conditioning while we watches the game on Saturday, where will it likely be installed?\ncar, house, offices, park, or movie theatre", "api_call_parameters": {"model": "text-davinci-002", "prompt": "A man wants air conditioning while we watches the game on Saturday, where will it likely be installed?\ncar, house, offices, park, or movie theatre\n\n", "stop": ["<|endoftext|>"], "temperature": 0, "logprobs": 1}, "gpt3_response": {"id": "cmpl-6SBoUJaJU1rKvwacIrAD5LDT2wLCj", "object": "text_completion", "created": 1672176614, "model": "text-davinci-002", "choices": [{"text": "\n\nThe man will likely install the air conditioning in his car or house.", "index": 0, "logprobs": {"tokens": ["\n", "\n", "The", " man", " will", " likely", " install", " the", " air", " conditioning", " in", " his", " car", " or", " house", "."], "token_logprobs": [-0.0011208493, -0.3923398, -1.0458204, -0.27530304, -0.34551388, -0.48221183, -0.17735067, -0.21305534, -3.2906587e-06, -0.009431251, -0.022507988, -0.002618181, -0.08245034, -0.69558316, -0.03406753, -0.017510267], "top_logprobs": [{"\n": -0.0011208493}, {"\n": -0.3923398}, {"The": -1.0458204}, {" man": -0.27530304}, {" will": -0.34551388}, {" likely": -0.48221183}, {" install": -0.17735067}, {" the": -0.21305534}, {" air": -3.2906587e-06}, {" conditioning": -0.009431251}, {" in": -0.022507988}, {" his": -0.002618181}, {" car": -0.08245034}, {" or": -0.69558316}, {" house": -0.03406753}, {".": -0.017510267}], "text_offset": [148, 149, 150, 153, 157, 162, 169, 177, 181, 185, 198, 201, 205, 209, 212, 218]}, "finish_reason": "length"}], "usage": {"prompt_tokens": 33, "completion_tokens": 16, "total_tokens": 49}}, "parsed_prediction": "car", "parsed_prediction_is_correct": 0, "parsed_prediction_response_type": "text", "total_logprob": -3.7968878169587, "prediction_as_answer_choice": "car", "prediction_as_answer_choice_index": 0, "num_shared_ngrams": [1, 1, 0, 0, 0]}
{"id": "3a3b5d4a517ef70d25eb558f1a622937", "question": "A patriotic guy with a camera is looking for a bald eagle, what is he likely to do with the eagle if he finds one?", "choices": ["city", "canada", "minnesota", "thermal", "photograph"], "answer": "photograph", "answer_index": 4, "prompt": "A patriotic guy with a camera is looking for a bald eagle, what is he likely to do with the eagle if he finds one?\ncity, canada, minnesota, thermal, or photograph", "api_call_parameters": {"model": "text-davinci-002", "prompt": "A patriotic guy with a camera is looking for a bald eagle, what is he likely to do with the eagle if he finds one?\ncity, canada, minnesota, thermal, or photograph\n\n", "stop": ["<|endoftext|>"], "temperature": 0, "logprobs": 1}, "gpt3_response": {"id": "cmpl-6SBoUOQ9KuXa6c8brmIOvxaKLT6Iz", "object": "text_completion", "created": 1672176614, "model": "text-davinci-002", "choices": [{"text": "\n\nIf the patriotic guy with a camera finds a bald eagle, he is", "index": 0, "logprobs": {"tokens": ["\n", "\n", "If", " the", " patriotic", " guy", " with", " a", " camera", " finds", " a", " bald", " eagle", ",", " he", " is"], "token_logprobs": [-0.0019496174, -0.67683303, -0.68573433, -0.34002078, -0.27442622, -0.0012244435, -0.22871564, -0.12424129, -3.154219e-05, -0.08478993, 0, -1.8073477e-05, -3.201996e-05, -0.00041244208, -0.0024122049, -0.017081209], "top_logprobs": [{"\n": -0.0019496174}, {"\n": -0.67683303}, {"If": -0.68573433}, {" the": -0.34002078}, {" patriotic": -0.27442622}, {" guy": -0.0012244435}, {" with": -0.22871564}, {" a": -0.12424129}, {" camera": -3.154219e-05}, {" finds": -0.08478993}, {" a": 0}, {" bald": -1.8073477e-05}, {" eagle": -3.201996e-05}, {",": -0.00041244208}, {" he": -0.0024122049}, {" is": -0.017081209}], "text_offset": [164, 165, 166, 168, 172, 182, 186, 191, 193, 200, 206, 208, 213, 219, 220, 223]}, "finish_reason": "length"}], "usage": {"prompt_tokens": 41, "completion_tokens": 16, "total_tokens": 57}}, "parsed_prediction": null, "parsed_prediction_is_correct": null, "parsed_prediction_response_type": null, "total_logprob": null, "prediction_as_answer_choice": null, "prediction_as_answer_choice_index": null, "num_shared_ngrams": [0, 0, 0, 0, 0]}
